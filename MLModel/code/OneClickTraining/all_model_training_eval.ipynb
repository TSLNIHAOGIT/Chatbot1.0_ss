{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T16:21:11.471027Z",
     "start_time": "2018-07-03T16:21:11.464442Z"
    }
   },
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# import lightgbm as lgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from all_model_py import CutDebt, IDClassifier, IfKnowDebtor, Installment, WillingToPay, ConfirmLoan\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('../CutDebt/')\n",
    "# sys.path.append('../IDClassifier/')\n",
    "# sys.path.append('../IfKnowDebtor/')\n",
    "# sys.path.append('../Installment/')\n",
    "# sys.path.append('../SetDueDay/')\n",
    "# sys.path.append('../WillingToPay/')\n",
    "\n",
    "# from CutDebt_py import CutDebt\n",
    "# from IDClassifier_py import IDClassifier\n",
    "# from IfKnowDebtor_py import IfKnowDebtor\n",
    "# from Installment_py import Installment\n",
    "# from SetDueDay_py import SetDueDay\n",
    "# from WillingToPay_py import WillingToPay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T16:21:13.630086Z",
     "start_time": "2018-07-03T16:21:12.041946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CutDebt\n",
      "0\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "start test\n",
      "IDClassifier\n",
      "0\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "start test\n",
      "IfKnowDebtor\n",
      "2\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "start test\n",
      "Installment\n",
      "0\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "start test\n",
      "ConfirmLoan\n",
      "1\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "start test\n",
      "WillingToPay\n",
      "2\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "start test\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "specific_accuracy = {} #record each parameted of the corresponding zero count\n",
    "accuracy=[] #record the threshold with highest accuracy\n",
    "result #the average probablity of three models\n",
    "label_result  #record the label with highest probablity\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_list = ['CutDebt','IDClassifier','IfKnowDebtor','Installment','ConfirmLoan','WillingToPay']\n",
    "func_list = [CutDebt,IDClassifier,IfKnowDebtor,Installment,ConfirmLoan,WillingToPay]\n",
    "\n",
    "accuracy=[]\n",
    "ind = 0\n",
    "specific_accuracy = {} #record each parameted of the corresponding zero count\n",
    "for each_model in model_list:\n",
    "    \n",
    "    print(each_model)\n",
    "    \n",
    "    # load data\n",
    "    path = '../../data/{}/'\n",
    "    data = pd.read_csv(path.format(each_model) + 'cleaned_mock_up_data.csv', encoding='utf8')\n",
    "    \n",
    "    \n",
    "    [data_train,data_test] = train_test_split(data, test_size = 0.1, shuffle=True)\n",
    "    print(data.loc[0,:].label)\n",
    "    \n",
    "#     data_train = data.get_chunk(5)\n",
    "#     data_test = pd.read_csv(path.format(each_model) + 'cleaned_mock_up_data.csv', encoding='utf8')\n",
    "    \n",
    "#     data_train = \n",
    "#     data_test = \n",
    "    \n",
    "    # get tfidf\n",
    "    phrase_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                    strip_accents='unicode', \n",
    "                                    max_features=100000, \n",
    "                                    analyzer='word',\n",
    "                                    sublinear_tf=True,\n",
    "                                    token_pattern=r'\\w{1,}')\n",
    "\n",
    "    print('fitting phrase')\n",
    "    phrase_vectorizer.fit(data_train.split_text)\n",
    "\n",
    "    print('transform phrase')\n",
    "    phrase = phrase_vectorizer.transform(data_train.split_text)\n",
    "\n",
    "    \n",
    "    # linear svc\n",
    "    l_svc = LinearSVC()\n",
    "    lsvc = CalibratedClassifierCV(l_svc) \n",
    "    lsvc.fit(phrase, data_train.label)\n",
    "    \n",
    "    \n",
    "    # logistic\n",
    "    log_r = LogisticRegression()\n",
    "    log_r.fit(phrase, data_train.label)\n",
    "    \n",
    "    \n",
    "    # Naive Bayes\n",
    "    naive_b = MultinomialNB()\n",
    "    naive_b.fit(phrase, data_train.label)\n",
    "    \n",
    "    \n",
    "    print('finish training')\n",
    "    \n",
    "    print('start test')\n",
    "    \n",
    "    text = phrase_vectorizer.transform(data_test.split_text)\n",
    "    temp1 = np.array(lsvc.predict_proba(text))\n",
    "    temp2 = np.array(log_r.predict_proba(text))\n",
    "    temp3 = np.array(naive_b.predict_proba(text))\n",
    "    result = np.zeros_like(temp1)\n",
    "    for i in range(len(temp1)):\n",
    "        result[i] = (temp1[i] + temp2[i] + temp3[i])/3\n",
    "    label_result = np.argmax(result,axis=1)\n",
    "    count = 0\n",
    "    \n",
    "    \n",
    "    test_index = len(data_test) - 1\n",
    "    test_label = data_test\n",
    "    max = 0 #zero count\n",
    "    accuracy_max = 0 #keep the most accuracte value\n",
    "    c = 0.2 \n",
    "    for x in range(0,60):\n",
    "        ind = (np.max(result, axis =1) < c)\n",
    "        result[ind,-1] = 1 \n",
    "        label_result = np.argmax(result,axis = 1)\n",
    "        temp = data_test['label'].values - label_result\n",
    "        accuracy_temp = temp.tolist().count(0) #current zero count\n",
    "        specific_accuracy[each_model+str(c)] = accuracy_temp\n",
    "        if accuracy_temp > max:\n",
    "            max = accuracy_temp\n",
    "            accuracy_max = c       \n",
    "        c += 0.01\n",
    "        \n",
    "    accuracy.append(accuracy_max)\n",
    "    \n",
    "        \n",
    "    \n",
    "                \n",
    "    \n",
    "    \n",
    "#     for threhold\n",
    "    \n",
    "    # save model\n",
    "#     save_path = '../../savedModel/{}/'\n",
    "#     # save tfidf\n",
    "#     pickle.dump(phrase_vectorizer, open(save_path.format(each_model) + \"tfidf.pickle\", \"wb\"))\n",
    "#     # save linear svc\n",
    "#     pickle.dump(lsvc, open(save_path.format(each_model) + \"LinearSVC.pickle\", \"wb\"))\n",
    "#     # save logistic\n",
    "#     pickle.dump(log_r, open(save_path.format(each_model) + \"Logistic.pickle\", \"wb\"))\n",
    "#     # save lightGBM\n",
    "# #     pickle.dump(lgbm_model, open(save_path.format(each_model) + \"Lgbm.pickle\", \"wb\"))\n",
    "#     # save naive bayes\n",
    "#     pickle.dump(naive_b, open(save_path.format(each_model) + \"nb.pickle\", \"wb\"))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     func = func_list[ind]\n",
    "#     result = func(svc=lsvc, logistic=log_r, nb=naive_b, tfidf=phrase_vectorizer, jieba_path='../WordCut/userdict.txt')\n",
    "#     pickle.dump(result, open(save_path.format(each_model) + each_model + '.pickle', \"wb\"))\n",
    "#     ind = ind + 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T16:22:02.558947Z",
     "start_time": "2018-07-03T16:22:02.543858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspecific_accuracy = {} #record each parameted of the corresponding zero count\\naccuracy=[] #record the threshold with highest accuracy\\nresult #the average probablity of three models\\nlabel_result  #record the label with highest probablity\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "specific_accuracy = {} #record each parameted of the corresponding zero count\n",
    "accuracy=[] #record the threshold with highest accuracy\n",
    "result #the average probablity of three models\n",
    "label_result  #record the label with highest probablity\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
