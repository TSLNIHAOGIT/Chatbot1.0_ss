{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T14:00:58.417803Z",
     "start_time": "2018-06-27T14:00:57.473683Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# import lightgbm as lgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import pickle\n",
    "\n",
    "from all_model_py import CutDebt, IDClassifier, IfKnowDebtor, Installment, WillingToPay, ConfirmLoan\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('../CutDebt/')\n",
    "# sys.path.append('../IDClassifier/')\n",
    "# sys.path.append('../IfKnowDebtor/')\n",
    "# sys.path.append('../Installment/')\n",
    "# sys.path.append('../SetDueDay/')\n",
    "# sys.path.append('../WillingToPay/')\n",
    "\n",
    "# from CutDebt_py import CutDebt\n",
    "# from IDClassifier_py import IDClassifier\n",
    "# from IfKnowDebtor_py import IfKnowDebtor\n",
    "# from Installment_py import Installment\n",
    "# from SetDueDay_py import SetDueDay\n",
    "# from WillingToPay_py import WillingToPay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T14:01:01.971800Z",
     "start_time": "2018-06-27T14:00:58.429373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CutDebt\n",
      "fitting phrase\n",
      "transform phrase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.830 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDClassifier\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "IfKnowDebtor\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "Installment\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "ConfirmLoan\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "WillingToPay\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n"
     ]
    }
   ],
   "source": [
    "model_list = ['CutDebt','IDClassifier','IfKnowDebtor','Installment','ConfirmLoan','WillingToPay']\n",
    "func_list = [CutDebt,IDClassifier,IfKnowDebtor,Installment,ConfirmLoan,WillingToPay]\n",
    "\n",
    "ind = 0\n",
    "for each_model in model_list:\n",
    "    print(each_model)\n",
    "    \n",
    "    # load data\n",
    "    path = '../../data/{}/'\n",
    "    data = pd.read_csv(path.format(each_model) + 'cleaned_mock_up_data.csv', encoding='utf8')\n",
    "    \n",
    "    # get tfidf\n",
    "    phrase_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                    strip_accents='unicode', \n",
    "                                    max_features=100000, \n",
    "                                    analyzer='word',\n",
    "                                    sublinear_tf=True,\n",
    "                                    token_pattern=r'\\w{1,}')\n",
    "\n",
    "    print('fitting phrase')\n",
    "    phrase_vectorizer.fit(data.split_text)\n",
    "\n",
    "    print('transform phrase')\n",
    "    phrase = phrase_vectorizer.transform(data.split_text)\n",
    "\n",
    "    \n",
    "    # linear svc\n",
    "    l_svc = LinearSVC()\n",
    "    lsvc = CalibratedClassifierCV(l_svc) \n",
    "    lsvc.fit(phrase, data.label)\n",
    "    \n",
    "    \n",
    "    # logistic\n",
    "    log_r = LogisticRegression()\n",
    "    log_r.fit(phrase, data.label)\n",
    "    \n",
    "    \n",
    "#     # lightGBM\n",
    "#     le = preprocessing.LabelEncoder()\n",
    "#     le.fit(data.label.values)\n",
    "#     onelabels = le.transform(data.label.values)\n",
    "#     multicoder = MultiLabelBinarizer()\n",
    "#     lables = multicoder.fit_transform([data.label.values])\n",
    "#     if each_model == 'WillingToPay':\n",
    "#         params = {\n",
    "#         'learning_rate': 0.2,\n",
    "#         'num_iterations':1000,\n",
    "#         'application': 'multiclassova',\n",
    "#         'num_class': 4,\n",
    "#         'num_leaves': 31,\n",
    "#         'verbosity': -1,\n",
    "#         'metric': 'multi_error',\n",
    "#         'data_random_seed': 2,\n",
    "#     #     'bagging_fraction': 0.8,\n",
    "#     #     'feature_fraction': 0.6,\n",
    "#         'nthread': 4,\n",
    "#         'lambda_l1': 1,\n",
    "#         'lambda_l2': 1,\n",
    "#         'early_stopping_round':200\n",
    "#         } \n",
    "#     else:        \n",
    "#         params = {\n",
    "#         'learning_rate': 0.2,\n",
    "#         'num_iterations':1000,\n",
    "#         'application': 'multiclassova',\n",
    "#         'num_class': 3,\n",
    "#         'num_leaves': 31,\n",
    "#         'verbosity': -1,\n",
    "#         'metric': 'multi_error',\n",
    "#         'data_random_seed': 2,\n",
    "#     #     'bagging_fraction': 0.8,\n",
    "#     #     'feature_fraction': 0.6,\n",
    "#         'nthread': 4,\n",
    "#         'lambda_l1': 1,\n",
    "#         'lambda_l2': 1,\n",
    "#         'early_stopping_round':200\n",
    "#         } \n",
    "#     # lgbm_train = lgb.Dataset(phrase, data.label)\n",
    "#     lgbm_train = lgb.Dataset(phrase, onelabels)\n",
    "#     lgbm_val = lgb.Dataset(phrase, onelabels)\n",
    "#     lgbm_model = lgb.train(params,lgbm_train,valid_sets=lgbm_val, verbose_eval=5)\n",
    "    \n",
    "    \n",
    "    # Naive Bayes\n",
    "    naive_b = MultinomialNB()\n",
    "    naive_b.fit(phrase, data.label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('finish training')\n",
    "    \n",
    "    # save model\n",
    "    save_path = '../../savedModel/{}/'\n",
    "    # save tfidf\n",
    "    pickle.dump(phrase_vectorizer, open(save_path.format(each_model) + \"tfidf.pickle\", \"wb\"))\n",
    "    # save linear svc\n",
    "    pickle.dump(lsvc, open(save_path.format(each_model) + \"LinearSVC.pickle\", \"wb\"))\n",
    "    # save logistic\n",
    "    pickle.dump(log_r, open(save_path.format(each_model) + \"Logistic.pickle\", \"wb\"))\n",
    "    # save lightGBM\n",
    "#     pickle.dump(lgbm_model, open(save_path.format(each_model) + \"Lgbm.pickle\", \"wb\"))\n",
    "    # save naive bayes\n",
    "    pickle.dump(naive_b, open(save_path.format(each_model) + \"nb.pickle\", \"wb\"))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    func = func_list[ind]\n",
    "    result = func(svc=lsvc, logistic=log_r, nb=naive_b, tfidf=phrase_vectorizer, jieba_path='../WordCut/userdict.txt')\n",
    "    pickle.dump(result, open(save_path.format(each_model) + each_model + '.pickle', \"wb\"))\n",
    "    ind = ind + 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T14:01:04.289567Z",
     "start_time": "2018-06-27T14:01:04.181321Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idc = pickle.load(open(\"../../savedModel/IDClassifier/IDClassifier.pickle\", 'rb'))\n",
    "cutd = pickle.load(open(\"../../savedModel/CutDebt/CutDebt.pickle\", 'rb'))\n",
    "ifk = pickle.load(open(\"../../savedModel/IfKnowDebtor/IfKnowDebtor.pickle\", 'rb'))\n",
    "will = pickle.load(open(\"../../savedModel/WillingToPay/WillingToPay.pickle\", 'rb'))\n",
    "inst = pickle.load(open(\"../../savedModel/Installment/Installment.pickle\", 'rb'))\n",
    "conf = pickle.load(open(\"../../savedModel/ConfirmLoan/ConfirmLoan.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T14:03:13.413772Z",
     "start_time": "2018-06-27T14:03:13.403426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, array([[ 0.05633146,  0.60015656,  0.25797365,  0.08553834],\n",
       "        [ 0.07754398,  0.55340964,  0.29572336,  0.07332302],\n",
       "        [ 0.06627425,  0.66367828,  0.24055026,  0.02949721]]), array([ 0.06671656,  0.60574816,  0.26474909,  0.06278619]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "will.classify('老子不还了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
