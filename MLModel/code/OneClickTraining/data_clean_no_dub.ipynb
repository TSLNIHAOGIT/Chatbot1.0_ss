{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T17:48:34.137843Z",
     "start_time": "2018-07-10T17:48:33.127345Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.468 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#encoding=utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.translate import NotTranslated\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "\n",
    "import jieba\n",
    "jieba.load_userdict(\"../WordCut/userdict.txt\")\n",
    "\n",
    "import gc\n",
    "# from googleapiclient.discovery import build\n",
    "import sys,os\n",
    "model_list = ['CutDebt','IDClassifier','IfKnowDebtor','Installment','WillingToPay','ConfirmLoan',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T17:48:35.778890Z",
     "start_time": "2018-07-10T17:48:35.746985Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('../TimePattern/')\n",
    "from  time_pattern import TimePattern\n",
    "t = TimePattern('../TimePattern/mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T17:48:42.388018Z",
     "start_time": "2018-07-10T17:48:42.374453Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate(comment, from_lang, to_lang):\n",
    "        if hasattr(comment, \"decode\"):\n",
    "            comment = comment.decode(\"utf-8\")\n",
    "\n",
    "        text = TextBlob(comment)\n",
    "        try:\n",
    "            text = text.translate(to=to_lang)\n",
    "            text = text.translate(to=from_lang)\n",
    "        except NotTranslated:\n",
    "            pass\n",
    "\n",
    "        return str(text)\n",
    "    \n",
    "\n",
    "\n",
    "def translate_csv(df,col,from_lang,to_lang,num_pol=10):\n",
    "    \"\"\"\n",
    "        https://developers.google.com/translate/v2/using_rest#language-params\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    if num_pol >= 1000:\n",
    "        num_pol=1000\n",
    "    comment_pool = df[col].values\n",
    "    p = Pool(num_pol)\n",
    "    new_col_name = col + '_' + to_lang\n",
    "    df[new_col_name] = p.starmap(translate, zip(comment_pool, repeat(from_lang),repeat(to_lang)))\n",
    "    df = df.drop([col], axis = 1)\n",
    "    df = df.rename(index=str, columns={new_col_name:col})\n",
    "    return df\n",
    "\n",
    "def cut_words(text):\n",
    "    ##### more -- added by wei\n",
    "    # this is used to remove time patterns from sentence\n",
    "    text = re.sub(r' ','',text)\n",
    "    text = t.remove_time(text)\n",
    "    #########\n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    return \" \".join(seg_list)\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(f'([{string.punctuation}“”¨«»®´·º ½¾¿¡§£₤‘’，])',' ', text)\n",
    "    text = text.split(' ')\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def clean_label(label):\n",
    "    return int(label)\n",
    "\n",
    "\n",
    "def get_others(other_data, label):\n",
    "    '''\n",
    "    this script take out all data which have the same label in the input label from the other data\n",
    "    other_data: Dataframe\n",
    "    label: list of all possible labels\n",
    "    return a Dataframe\n",
    "    '''\n",
    "    result = pd.Dataframe()\n",
    "    for l in label:\n",
    "        temp = other_data[other_data.label == l]\n",
    "        result = pd.concat([result, temp], ignore_index=True)\n",
    "       \n",
    "    return result\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T17:48:43.198660Z",
     "start_time": "2018-07-10T17:48:43.193375Z"
    }
   },
   "outputs": [],
   "source": [
    "#####\n",
    "#########################################\n",
    "#Goold Official Translation API\n",
    "#Need to Pay\n",
    "#############################################\n",
    "\n",
    "#################\n",
    "\n",
    "# # service = build('translate', 'v2', developerKey='AIzaSyD2ZNI7MBVVFdMnHse7w0asv2R3LyEFMf4')    # Jiangning\n",
    "# import html\n",
    "\n",
    "# service = build('translate', 'v2', developerKey='AIzaSyD2ZNI7MBVVFdMnHse7w0asv2R3LyEFMf4')\n",
    "\n",
    "# def translate2(values, from_lang, to_lang):\n",
    "#         if not isinstance(values, list):\n",
    "#             values = list(values)\n",
    "#         response = service.translations().list(source=from_lang,\n",
    "#                           target=to_lang, q=values).execute()\n",
    "#         processed = []\n",
    "#         for each in response['translations']:\n",
    "#             processed.append(each['translatedText'])\n",
    "#         processed = list(map(lambda x: html.unescape(x), processed))\n",
    "        \n",
    "#         # translate back\n",
    "#         response = service.translations().list(source=to_lang,\n",
    "#                           target=from_lang, q=processed).execute()\n",
    "#         processed = []\n",
    "#         for each in response['translations']:\n",
    "#             processed.append(each['translatedText'])\n",
    "#         return processed\n",
    "    \n",
    "    \n",
    "# def translate_csv(df,col,from_lang,to_lang,num_pol=None):\n",
    "#     \"\"\"\n",
    "#         https://developers.google.com/translate/v2/using_rest#language-params\n",
    "#     \"\"\"\n",
    "\n",
    "#     df = df.copy()\n",
    "#     new_col_name = col + '_' + to_lang\n",
    "#     df[new_col_name] = translate2(df[col].values, from_lang, to_lang)\n",
    "# #     df = df.drop([col], axis = 1)\n",
    "# #     df = df.rename(index=str, columns={new_col_name:col})\n",
    "#     return df\n",
    "\n",
    "# # c= translate_csv(data,col='split_text' ,from_lang='zh', to_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T17:48:46.788763Z",
     "start_time": "2018-07-10T17:48:46.650385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this part combine the data which can be used together.\n",
    "for each_model in model_list:\n",
    "    path = '../../data/{}/'\n",
    "    data = pd.read_csv(path.format(each_model) + 'mock_up_data1.csv', encoding='utf8')\n",
    "    data.to_csv(path.format(each_model) + 'combined_mock_up_data.csv', index = False, encoding = 'utf8')\n",
    "\n",
    "# label 0 part for CutDebt and Installment\n",
    "data_cut = pd.read_csv(path.format('CutDebt') + 'combined_mock_up_data.csv', encoding = 'utf8')\n",
    "data_ins = pd.read_csv(path.format('Installment') + 'combined_mock_up_data.csv', encoding = 'utf8')\n",
    "temp_cut = data_cut[data_cut.label == 0]\n",
    "temp_ins = data_ins[data_ins.label == 0]\n",
    "data_cut = pd.concat([data_cut,temp_ins], ignore_index=True)\n",
    "data_ins = pd.concat([data_ins,temp_cut], ignore_index=True)\n",
    "data_cut.to_csv(path.format('CutDebt') + 'combined_mock_up_data.csv', index = False, encoding = 'utf8')\n",
    "data_ins.to_csv(path.format('Installment') + 'combined_mock_up_data.csv', index = False, encoding = 'utf8')\n",
    "\n",
    "del data_cut\n",
    "del data_ins\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T17:49:49.937022Z",
     "start_time": "2018-07-10T17:49:48.791409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CutDebt\n",
      "finish cutting words\n",
      "finish shuffling\n",
      "finish dropping dupblicates\n",
      "IDClassifier\n",
      "finish cutting words\n",
      "finish shuffling\n",
      "finish dropping dupblicates\n",
      "IfKnowDebtor\n",
      "finish cutting words\n",
      "finish shuffling\n",
      "finish dropping dupblicates\n",
      "Installment\n",
      "finish cutting words\n",
      "finish shuffling\n",
      "finish dropping dupblicates\n",
      "WillingToPay\n",
      "finish cutting words\n",
      "finish shuffling\n",
      "finish dropping dupblicates\n",
      "ConfirmLoan\n",
      "finish cutting words\n",
      "finish shuffling\n",
      "finish dropping dupblicates\n"
     ]
    }
   ],
   "source": [
    "# combine without translate\n",
    "for each_model in model_list:\n",
    "    print(each_model)\n",
    "    path = '../../data/{}/'\n",
    "    data = pd.read_csv(path.format(each_model) + 'combined_mock_up_data.csv', encoding = 'utf8')\n",
    "    data = data.dropna()\n",
    "    col = 'split_text'\n",
    "    \n",
    "    # cut words\n",
    "    data['split_text']=data['split_text'].apply(cut_words)\n",
    "    print('finish cutting words')\n",
    "    \n",
    "    # cleaning and save\n",
    "    data['split_text'] = data['split_text'].apply(clean)\n",
    "    data['label'] = data['label'].apply(clean_label)\n",
    "\n",
    "    # shuffle data\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    print('finish shuffling')\n",
    "    \n",
    "    # take out dublicates\n",
    "    data = data.drop_duplicates()\n",
    "    print('finish dropping dupblicates')\n",
    "    \n",
    "    data.to_csv(path.format(each_model) + 'cleaned_mock_up_data_no_dub.csv', index = False, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T20:27:24.126436Z",
     "start_time": "2018-06-29T20:27:24.114574Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each_model in model_list:\n",
    "#     print(each_model)\n",
    "#     path = '../../data/{}/'\n",
    "#     data = pd.read_csv(path.format(each_model) + 'combined_mock_up_data.csv', encoding='utf8')\n",
    "#     data = data.dropna()\n",
    "#     col = 'split_text'\n",
    "#     print('finish loading')\n",
    "    \n",
    "#     # translate and get more data\n",
    "#     data_en = translate_csv(data,col,from_lang='zh',to_lang='en',num_pol=50)\n",
    "#     print('finish 1st trans')\n",
    "#     data_fr = translate_csv(data,col,from_lang='zh',to_lang='fr',num_pol=50)\n",
    "#     print('finish 2nd trans')\n",
    "#     data_th = translate_csv(data,col,from_lang='zh',to_lang='th',num_pol=50)\n",
    "#     print('finish 3rd trans')\n",
    "#     data_lo = translate_csv(data,col,from_lang='zh',to_lang='lo',num_pol=50)\n",
    "#     print('finish 4th trans')\n",
    "#     data = pd.concat([data,data_en,data_fr,data_th,data_lo], ignore_index=True)\n",
    "# #     data = pd.concat([data,data_en,data_fr], ignore_index=True)\n",
    "    \n",
    "#     # cut words\n",
    "#     data['split_text']=data['split_text'].apply(cut_words)\n",
    "#     print('finish cutting words')\n",
    "    \n",
    "#     # cleaning and save\n",
    "#     data['split_text'] = data['split_text'].apply(clean)\n",
    "#     data['label'] = data['label'].apply(clean_label)\n",
    "\n",
    "#     # shuffle data\n",
    "#     data = data.sample(frac=1).reset_index(drop=True)\n",
    "#     print('finish shuffling')\n",
    "#     data.to_csv(path.format(each_model) + 'cleaned_mock_up_data.csv', index = False, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T14:56:10.877407Z",
     "start_time": "2018-06-26T14:56:10.815645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2751, 2)\n",
      "(2830, 2)\n",
      "(3763, 2)\n",
      "(2832, 2)\n",
      "(3278, 3)\n",
      "(3285, 2)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T16:02:13.387945Z",
     "start_time": "2018-06-26T16:02:13.358378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3278, 3)\n",
      "(3285, 2)\n",
      "(15, 2)\n"
     ]
    }
   ],
   "source": [
    "# # this script combine data in combine_list and 'setdueday' together to get new dataset\n",
    "\n",
    "# combine_list =  ['CutDebt','Installment','WillingToPay']\n",
    "\n",
    "# for each_model in combine_list:\n",
    "#     path = '../../data/{}/'\n",
    "#     SDD_data = pd.read_csv(path.format('SetDueDay') + 'mock_up_data.csv', encoding='utf8')\n",
    "#     data = pd.read_csv(path.format(each_model) + 'mock_up_data.csv', encoding='utf8')\n",
    "#     data_comb = pd.concat([data, SDD_data], ignore_index=True)\n",
    "#     print(data_comb.shape)\n",
    "#     data_comb.to_csv(path.format(each_model) + 'mock_up_data1.csv', index = False, encoding = 'utf8')\n",
    "\n",
    "\n",
    "# combine_list =  ['IDClassifier', 'IfKnowDebtor', 'ConfirmLoan']\n",
    "\n",
    "# for each_model in combine_list:\n",
    "#     path = '../../data/{}/'\n",
    "# #     SDD_data = pd.read_csv(path.format('SetDueDay') + 'mock_up_data.csv', encoding='utf8')\n",
    "#     data = pd.read_csv(path.format(each_model) + 'mock_up_data.csv', encoding='utf8')\n",
    "# #     data_comb = pd.concat([data, SDD_data], ignore_index=True)\n",
    "#     print(data.shape)\n",
    "#     data.to_csv(path.format(each_model) + 'mock_up_data1.csv', index = False, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
