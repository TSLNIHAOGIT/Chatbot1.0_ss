{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-30T22:59:58.215919Z",
     "start_time": "2018-08-30T22:59:56.081626Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.918 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#encoding=utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.translate import NotTranslated\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "from tqdm import tqdm\n",
    "import jieba\n",
    "jieba.load_userdict(\"../WordCut/userdict.txt\")\n",
    "\n",
    "import gc\n",
    "# from googleapiclient.discovery import build\n",
    "import sys,os\n",
    "model_list = ['CutDebt','IDClassifier','IfKnowDebtor','Installment','WillingToPay','ConfirmLoan',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-30T22:59:58.329237Z",
     "start_time": "2018-08-30T22:59:58.221970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Zone is set from ENV: Asia/Shanghai\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('../../../classifier/models/time_pattern/')\n",
    "from  time_pattern import TimePattern\n",
    "t = TimePattern()\n",
    "\n",
    "\n",
    "\n",
    "def cut_words(text):\n",
    "    ##### more -- added by wei\n",
    "    # this is used to remove time patterns from sentence\n",
    "    text = re.sub(r' ','TIMESERIES ',text)\n",
    "    text = t.remove_time(text)\n",
    "    #########\n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    return \" \".join(seg_list)\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(f'([{string.punctuation}“”¨«»®´·º ½¾¿¡§£₤‘’，])',' ', text)\n",
    "    text = text.split(' ')\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def clean_label(label):\n",
    "    return int(label)\n",
    "\n",
    "\n",
    "def load_others(classifier,\n",
    "                label_list,\n",
    "                other_fe = ['text','label'],\n",
    "                other_path = '../../data/others/labels/{}/mock_up_data_new.csv'):\n",
    "    \"\"\"\n",
    "    classifier: eg, CutDebt\n",
    "    label_list: eg, [102, 103, 104, 106, 107, 108, 109, 110]\n",
    "    \"\"\"\n",
    "    others = pd.DataFrame()\n",
    "    for label in label_list:\n",
    "\n",
    "        df_load = pd.read_csv(other_path.format(label))\n",
    "        df_availabel = df_load[df_load[classifier] == 0][other_fe].copy()\n",
    "        others = pd.concat([others,df_availabel],ignore_index=True)\n",
    "    return others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-30T22:59:58.478076Z",
     "start_time": "2018-08-30T22:59:58.332905Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 50.52it/s]\n"
     ]
    }
   ],
   "source": [
    "features = ['label','split_text']\n",
    "ori_data_main = {}\n",
    "for each_model in tqdm(model_list):\n",
    "    path = '../../data/{}/'\n",
    "    ori_data_main[each_model] = pd.read_csv(path.format(each_model) + 'mock_up_data_clean_new.csv', encoding='utf8')\n",
    "    ori_data_main[each_model] = ori_data_main[each_model][features]\n",
    "    \n",
    "#combine CUtDebt and Installment label 0\n",
    "cut_0 = ori_data_main['CutDebt'][ori_data_main['CutDebt'].label == 0].copy()\n",
    "ins_0 = ori_data_main['Installment'][ori_data_main['Installment'].label == 0].copy()\n",
    "\n",
    "ori_data_main['CutDebt'] = pd.concat([ori_data_main['CutDebt'],ins_0],ignore_index=True)\n",
    "ori_data_main['Installment'] = pd.concat([ori_data_main['Installment'],cut_0],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Other Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-30T22:59:58.939397Z",
     "start_time": "2018-08-30T22:59:58.483137Z"
    }
   },
   "outputs": [],
   "source": [
    "### get others data\n",
    "strategy_mat = pd.read_csv('../../data/others/strategy_mat_v1.csv')\n",
    "ori_data_other = {}\n",
    "for each_model in model_list:\n",
    "    available_labels = list(strategy_mat[strategy_mat[each_model]==0]['label'].unique())\n",
    "    ori_data_other[each_model] = load_others(each_model,available_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-30T23:00:01.542258Z",
     "start_time": "2018-08-30T22:59:58.942230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CutDebt\n",
      "finish cutting words\n",
      "IDClassifier\n",
      "finish cutting words\n",
      "IfKnowDebtor\n",
      "finish cutting words\n",
      "Installment\n",
      "finish cutting words\n",
      "WillingToPay\n",
      "finish cutting words\n",
      "ConfirmLoan\n",
      "finish cutting words\n"
     ]
    }
   ],
   "source": [
    "clean_data_main = {}\n",
    "clean_data_other = {}\n",
    "for each_model in model_list:\n",
    "    print(each_model)\n",
    "\n",
    "    clean_data_main[each_model] = ori_data_main[each_model].dropna()\n",
    "    clean_data_other[each_model] = ori_data_other[each_model].dropna()\n",
    "    col = 'split_text'\n",
    "    col_other = 'text'\n",
    "    # cut words\n",
    "    clean_data_main[each_model][col]=clean_data_main[each_model][col].apply(cut_words)\n",
    "    clean_data_other[each_model][col_other]=clean_data_other[each_model][col_other].apply(cut_words)\n",
    "    print('finish cutting words')\n",
    "    \n",
    "    # cleaning and save\n",
    "    clean_data_main[each_model][col] = clean_data_main[each_model][col].apply(clean)\n",
    "    clean_data_other[each_model][col_other] = clean_data_other[each_model][col_other].apply(clean)\n",
    "    \n",
    "    clean_data_main[each_model]['label'] = clean_data_main[each_model]['label'].apply(clean_label)\n",
    "    clean_data_other[each_model]['label'] = clean_data_other[each_model]['label'].apply(clean_label)\n",
    "\n",
    "    # shuffle data\n",
    "    clean_data_main[each_model] = clean_data_main[each_model].sample(frac=1).reset_index(drop=True)\n",
    "    clean_data_other[each_model] = clean_data_other[each_model].sample(frac=1).reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-30T23:00:01.605478Z",
     "start_time": "2018-08-30T23:00:01.545167Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle\n",
    "\n",
    "import sys,os\n",
    "models_path = '../../../classifier/models/ml_models/'\n",
    "sys.path.append(models_path)\n",
    "from ml import *\n",
    "\n",
    "\n",
    "\n",
    "model_list = {\n",
    "                'IDClassifier':IDClassifier, \n",
    "                  'CutDebt':CutDebt, \n",
    "                  'WillingToPay':WillingToPay,\n",
    "                  'IfKnowDebtor':IfKnowDebtor,\n",
    "                  'Installment':Installment,\n",
    "                  'ConfirmLoan':ConfirmLoan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-30T23:00:01.621333Z",
     "start_time": "2018-08-30T23:00:01.608595Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_other_model(other_data,save_path,model):\n",
    "    phrase_vectorizer_other = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                strip_accents='unicode', \n",
    "                                max_features=100000, \n",
    "                                analyzer='word',\n",
    "                                sublinear_tf=True,\n",
    "                                token_pattern=r'\\w{1,}')\n",
    "\n",
    "    print('fitting phrase')\n",
    "    phrase_vectorizer_other.fit(other_data.text)\n",
    "\n",
    "    print('transform phrase')\n",
    "    phrase = phrase_vectorizer_other.transform(other_data.text)\n",
    "\n",
    "\n",
    "    # linear svc\n",
    "    l_svc = LinearSVC()\n",
    "    lsvc = CalibratedClassifierCV(l_svc) \n",
    "    lsvc.fit(phrase, other_data.label)\n",
    "\n",
    "\n",
    "    # logistic\n",
    "    log_r = LogisticRegression()\n",
    "    log_r.fit(phrase, other_data.label)\n",
    "\n",
    "\n",
    "    # Naive Bayes\n",
    "    naive_b = MultinomialNB()\n",
    "    naive_b.fit(phrase, other_data.label)\n",
    "    \n",
    "    print('finish training others')\n",
    "    \n",
    "    \n",
    "    # other wrapper \n",
    "    other_model = ClassifierOther(svc=lsvc, logistic=log_r, nb=naive_b, tfidf=phrase_vectorizer_other, jieba_path='../WordCut/userdict.txt',possible_label=lsvc.classes_)\n",
    "    \n",
    "    # Saving:\n",
    "    evl_path = save_path.format(model,model)\n",
    "    print('saving to path: {}'.format(evl_path))\n",
    "    pickle.dump(other_model, open(evl_path, \"wb\"))\n",
    "    return other_model\n",
    "    \n",
    "    \n",
    "def train_main_model(df,save_path,model,other_model):\n",
    "    # get tfidf\n",
    "    \n",
    "    phrase_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                    strip_accents='unicode', \n",
    "                                    max_features=100000, \n",
    "                                    analyzer='word',\n",
    "                                    sublinear_tf=True,\n",
    "                                    token_pattern=r'\\w{1,}')\n",
    "\n",
    "    print('fitting phrase')\n",
    "    phrase_vectorizer.fit(df.split_text)\n",
    "\n",
    "    print('transform phrase')\n",
    "    phrase = phrase_vectorizer.transform(df.split_text)\n",
    "    \n",
    "    # linear svc\n",
    "    l_svc = LinearSVC()\n",
    "    lsvc = CalibratedClassifierCV(l_svc) \n",
    "    lsvc.fit(phrase, df.label)\n",
    "    \n",
    "    \n",
    "    # logistic\n",
    "    log_r = LogisticRegression()\n",
    "    log_r.fit(phrase, df.label)\n",
    "    \n",
    "    \n",
    "    # Naive Bayes\n",
    "    naive_b = MultinomialNB()\n",
    "    naive_b.fit(phrase, df.label)\n",
    "    print('finish training')\n",
    "    \n",
    "    main_model = model_list[model](svc=lsvc, logistic=log_r, nb=naive_b, tfidf=phrase_vectorizer, other=other_model,  jieba_path='../WordCut/userdict.txt')\n",
    "    evl_path = save_path.format(model,model)\n",
    "    pickle.dump(main_model, open(evl_path, \"wb\"))\n",
    "    print('saving to path: {}'.format(evl_path))\n",
    "    return main_model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Other Model + Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-30T23:00:10.938913Z",
     "start_time": "2018-08-30T23:00:01.629179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting phrase\n",
      "transform phrase\n",
      "finish training others\n",
      "saving to path: ../../../classifier/saved_model/IDClassifier/other_flow/IDClassifier.pkl\n",
      "=====  IDClassifier =======\n",
      "2    3277\n",
      "1     530\n",
      "0     337\n",
      "Name: label, dtype: int64\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "saving to path: ../../../classifier/saved_model/IDClassifier/main_flow/IDClassifier.pkl\n",
      "\n",
      "\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training others\n",
      "saving to path: ../../../classifier/saved_model/CutDebt/other_flow/CutDebt.pkl\n",
      "=====  CutDebt =======\n",
      "2    4619\n",
      "1    1432\n",
      "0    1364\n",
      "Name: label, dtype: int64\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "Time Zone is set from ENV: Asia/Shanghai\n",
      "saving to path: ../../../classifier/saved_model/CutDebt/main_flow/CutDebt.pkl\n",
      "\n",
      "\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training others\n",
      "saving to path: ../../../classifier/saved_model/WillingToPay/other_flow/WillingToPay.pkl\n",
      "=====  WillingToPay =======\n",
      "2    4839\n",
      "1    1945\n",
      "0     663\n",
      "Name: label, dtype: int64\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "Time Zone is set from ENV: Asia/Shanghai\n",
      "saving to path: ../../../classifier/saved_model/WillingToPay/main_flow/WillingToPay.pkl\n",
      "\n",
      "\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training others\n",
      "saving to path: ../../../classifier/saved_model/IfKnowDebtor/other_flow/IfKnowDebtor.pkl\n",
      "=====  IfKnowDebtor =======\n",
      "2    3273\n",
      "0     892\n",
      "1     521\n",
      "Name: label, dtype: int64\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "saving to path: ../../../classifier/saved_model/IfKnowDebtor/main_flow/IfKnowDebtor.pkl\n",
      "\n",
      "\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training others\n",
      "saving to path: ../../../classifier/saved_model/Installment/other_flow/Installment.pkl\n",
      "=====  Installment =======\n",
      "2    4632\n",
      "1    1368\n",
      "0    1364\n",
      "Name: label, dtype: int64\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "Time Zone is set from ENV: Asia/Shanghai\n",
      "saving to path: ../../../classifier/saved_model/Installment/main_flow/Installment.pkl\n",
      "\n",
      "\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training others\n",
      "saving to path: ../../../classifier/saved_model/ConfirmLoan/other_flow/ConfirmLoan.pkl\n",
      "=====  ConfirmLoan =======\n",
      "2    3300\n",
      "0    1157\n",
      "1     607\n",
      "Name: label, dtype: int64\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n",
      "Time Zone is set from ENV: Asia/Shanghai\n",
      "saving to path: ../../../classifier/saved_model/ConfirmLoan/main_flow/ConfirmLoan.pkl\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# each_model = 'IDClassifier' \n",
    "save_path_other = '../../../classifier/saved_model/{}/other_flow/{}.pkl'\n",
    "save_path_main = '../../../classifier/saved_model/{}/main_flow/{}.pkl'\n",
    "for each_model in model_list:\n",
    "   \n",
    "\n",
    "    other_model = train_other_model(clean_data_other[each_model],save_path_other,each_model)\n",
    "    \n",
    "    df_main = clean_data_main[each_model].copy()\n",
    "    other_label = int(max(set(df_main.label)) + 1)\n",
    "    ava_others = clean_data_other[each_model].rename({'text':'split_text'},axis=1).copy()\n",
    "    ava_others['label'] = other_label\n",
    "    df_main = pd.concat([df_main,ava_others],sort=True)\n",
    "    df_main = df_main.sample(frac=1,random_state=6).reset_index(drop=True)\n",
    "    print('=====  {} ======='.format(each_model))\n",
    "    print(df_main.label.value_counts())\n",
    "    clf = train_main_model(df_main,save_path_main,each_model,other_model)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
