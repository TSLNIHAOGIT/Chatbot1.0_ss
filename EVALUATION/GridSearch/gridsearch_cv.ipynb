{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T21:09:59.169315Z",
     "start_time": "2018-09-11T21:09:56.710674Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.046 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sys,os\n",
    "\n",
    "sys.path.append('../../classifier/models/ml_models')\n",
    "from ml import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import jieba\n",
    "jieba_path = \"../../MLModel/code/WordCut/userdict.txt\"\n",
    "jieba.load_userdict(jieba_path)\n",
    "import gc\n",
    "\n",
    "import sys,os\n",
    "sys.path.append('../../Lib/')\n",
    "from load_cleaned_data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T21:09:59.233069Z",
     "start_time": "2018-09-11T21:09:59.221620Z"
    }
   },
   "outputs": [],
   "source": [
    "model_list = {\n",
    "'IDClassifier':IDClassifier, \n",
    "'CutDebt':CutDebt, \n",
    "'WillingToPay':WillingToPay,\n",
    "'IfKnowDebtor':IfKnowDebtor,\n",
    "'Installment':Installment,\n",
    "'ConfirmLoan':ConfirmLoan\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T21:10:03.912002Z",
     "start_time": "2018-09-11T21:09:59.281807Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 40.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CutDebt\n",
      "finish cutting words\n",
      "1    1434\n",
      "0    1364\n",
      "Name: label, dtype: int64\n",
      "109    1376\n",
      "106     997\n",
      "104     907\n",
      "103     552\n",
      "108     355\n",
      "102     266\n",
      "107     133\n",
      "110      33\n",
      "Name: label, dtype: int64\n",
      "IDClassifier\n",
      "finish cutting words\n",
      "1    533\n",
      "0    339\n",
      "Name: label, dtype: int64\n",
      "109    1397\n",
      "104     952\n",
      "103     563\n",
      "107     366\n",
      "Name: label, dtype: int64\n",
      "IfKnowDebtor\n",
      "finish cutting words\n",
      "0    894\n",
      "1    519\n",
      "Name: label, dtype: int64\n",
      "109    1393\n",
      "104     952\n",
      "103     563\n",
      "107     365\n",
      "Name: label, dtype: int64\n",
      "Installment\n",
      "finish cutting words\n",
      "1    1368\n",
      "0    1364\n",
      "Name: label, dtype: int64\n",
      "109    1376\n",
      "106     998\n",
      "104     907\n",
      "103     553\n",
      "108     355\n",
      "102     277\n",
      "107     133\n",
      "110      33\n",
      "Name: label, dtype: int64\n",
      "WillingToPay\n",
      "finish cutting words\n",
      "1    1947\n",
      "0     669\n",
      "Name: label, dtype: int64\n",
      "109    1375\n",
      "106     988\n",
      "104     905\n",
      "103     551\n",
      "108     351\n",
      "102     334\n",
      "105     202\n",
      "107     133\n",
      "Name: label, dtype: int64\n",
      "ConfirmLoan\n",
      "finish cutting words\n",
      "0    1157\n",
      "1     609\n",
      "Name: label, dtype: int64\n",
      "109    1375\n",
      "104     900\n",
      "103     547\n",
      "108     344\n",
      "107     134\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# each_model = 'IDClassifier' \n",
    "clean_data_main,clean_data_other = load_data(load_fb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T21:10:04.286009Z",
     "start_time": "2018-09-11T21:10:04.258171Z"
    }
   },
   "outputs": [],
   "source": [
    "def getEvaluatingMatrix(val_y,val_preds):\n",
    "    cm = confusion_matrix(val_y,val_preds)\n",
    "    accuracy_total = (cm[0,0]+cm[1,1]+cm[2,2]) / np.sum(cm)\n",
    "\n",
    "    accuracy_01 = (cm[0,0]+cm[1,1]) / np.sum(cm[:2,:])\n",
    "\n",
    "    ###\n",
    "    label_0_precision = cm[0,0] / np.sum(cm[:,0])\n",
    "    label_0_recall = cm[0,0] / np.sum(cm[0,:])\n",
    "    label_0_count = np.sum(cm[0,:])\n",
    "    label_1_precision = cm[1,1] / np.sum(cm[:,1])\n",
    "    label_1_recall = cm[1,1] / np.sum(cm[1,:])\n",
    "    label_1_count = np.sum(cm[1,:])\n",
    "    label_2_precision = cm[2,2] / np.sum(cm[:,2])\n",
    "    label_2_recall = cm[2,2] / np.sum(cm[2,:])\n",
    "    label_2_count = np.sum(cm[2,:])\n",
    "    return (accuracy_total,\n",
    "            accuracy_01,\n",
    "            label_0_precision,\n",
    "            label_0_recall,\n",
    "            label_0_count,\n",
    "            label_1_precision,\n",
    "            label_1_recall,\n",
    "            label_1_count,\n",
    "            label_2_precision,\n",
    "            label_2_recall,\n",
    "            label_2_count)\n",
    "\n",
    "def train_other_model(other_data,ng_range=(1,3),C_svc=1,C_lgs=1,alpha_nb=1,):\n",
    "    phrase_vectorizer_other = TfidfVectorizer(ngram_range=ng_range,\n",
    "                                strip_accents='unicode', \n",
    "                                max_features=100000, \n",
    "                                analyzer='word',\n",
    "                                sublinear_tf=True,\n",
    "                                token_pattern=r'\\w{1,}')\n",
    "\n",
    "    print('fitting phrase')\n",
    "    phrase_vectorizer_other.fit(other_data.text)\n",
    "\n",
    "    print('transform phrase')\n",
    "    phrase = phrase_vectorizer_other.transform(other_data.text)\n",
    "\n",
    "\n",
    "    # linear svc\n",
    "    l_svc = LinearSVC(C=C_svc)\n",
    "    lsvc = CalibratedClassifierCV(l_svc) \n",
    "    lsvc.fit(phrase, other_data.label)\n",
    "\n",
    "\n",
    "    # logistic\n",
    "    log_r = LogisticRegression(C=C_lgs)\n",
    "    log_r.fit(phrase, other_data.label)\n",
    "\n",
    "\n",
    "    # Naive Bayes\n",
    "    naive_b = MultinomialNB(alpha=alpha_nb)\n",
    "    naive_b.fit(phrase, other_data.label)\n",
    "    \n",
    "    print('finish training others')\n",
    "    \n",
    "    \n",
    "    # other wrapper \n",
    "    other_model = ClassifierOther(svc=lsvc, logistic=log_r, nb=naive_b, tfidf=phrase_vectorizer_other, jieba_path=jieba_path,possible_label=lsvc.classes_)\n",
    "    \n",
    "    return other_model\n",
    "    \n",
    "    \n",
    "def train_main_model(df,model,other_model,ng_range=(1,3),C_svc=1,C_lgs=1,alpha_nb=1,weight_list=[]):\n",
    "    # get tfidf\n",
    "    \n",
    "    phrase_vectorizer = TfidfVectorizer(ngram_range=ng_range,\n",
    "                                    strip_accents='unicode', \n",
    "                                    max_features=100000, \n",
    "                                    analyzer='word',\n",
    "                                    sublinear_tf=True,\n",
    "                                    token_pattern=r'\\w{1,}')\n",
    "\n",
    "    print('fitting phrase')\n",
    "    phrase_vectorizer.fit(df.split_text)\n",
    "\n",
    "    print('transform phrase')\n",
    "    phrase = phrase_vectorizer.transform(df.split_text)\n",
    "    \n",
    "    # linear svc\n",
    "    l_svc = LinearSVC(C=C_svc)\n",
    "    lsvc = CalibratedClassifierCV(l_svc) \n",
    "    lsvc.fit(phrase, df.label)\n",
    "    \n",
    "    \n",
    "    # logistic\n",
    "    log_r = LogisticRegression(C=C_lgs)\n",
    "    log_r.fit(phrase, df.label)\n",
    "    \n",
    "    \n",
    "    # Naive Bayes\n",
    "    naive_b = MultinomialNB(alpha=alpha_nb)\n",
    "    naive_b.fit(phrase, df.label)\n",
    "    print('finish training')\n",
    "    \n",
    "    for each_w in weight_list:\n",
    "        main_model = model_list[model](svc=lsvc, logistic=log_r, nb=naive_b, tfidf=phrase_vectorizer, other=other_model,  jieba_path=jieba_path,weights=each_w)\n",
    "\n",
    "        yield main_model,each_w\n",
    "    \n",
    "    \n",
    "def get_mainVSothers_data(model,seed=6):\n",
    "    data_other = clean_data_other[model].copy()\n",
    "    df_main = clean_data_main[model].copy()\n",
    "    other_label = int(max(set(df_main.label)) + 1)\n",
    "    ava_others = data_other.rename({'text':'split_text'},axis=1).copy()\n",
    "    ava_others['label'] = other_label\n",
    "    df_main = pd.concat([df_main,ava_others],sort=True)\n",
    "    df_main = df_main.sample(frac=1,random_state=seed).reset_index(drop=True)\n",
    "    data_other = data_other.sample(frac=1,random_state=seed).reset_index(drop=True)\n",
    "    return df_main,data_other\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T21:10:04.630530Z",
     "start_time": "2018-09-11T21:10:04.609396Z"
    }
   },
   "outputs": [],
   "source": [
    "def cv_run(data_main,weight_list,nr,C_svc,C_lgs,alpha,nFold=5,seed=19):\n",
    "    sf = StratifiedKFold(n_splits=nFold,shuffle=True,random_state=seed)\n",
    "    train_index_list = []\n",
    "    val_index_list = []\n",
    "    for train_index,val_index in sf.split(data_main['split_text'],data_main['label']):\n",
    "        train_index_list.append(train_index)\n",
    "        val_index_list.append(val_index)\n",
    "    ##############################################################\n",
    "    overall_p = []\n",
    "    for fold in range(nFold):\n",
    "        print(fold)\n",
    "        fold_p = []\n",
    "        train_data = data_main.iloc[train_index_list[fold]].copy()\n",
    "        val_data = data_main.iloc[val_index_list[fold]].copy()\n",
    "        val_x = val_data['split_text'].values\n",
    "        val_y = val_data['label'].values\n",
    "        for main_model,w in train_main_model(train_data,\n",
    "                                      model=model,\n",
    "                                      ng_range=(1,nr),\n",
    "                                      other_model=other_model,\n",
    "                                      C_svc=C_svc,\n",
    "                                      C_lgs=C_lgs,\n",
    "                                      alpha_nb=alpha,\n",
    "                                      weight_list=weight_list):\n",
    "            val_preds = []\n",
    "            print(w)\n",
    "            for x in val_x:\n",
    "                val_preds.append(main_model.classify(x)['ml_label'])\n",
    "            val_preds = np.array(val_preds)\n",
    "            val_preds[val_preds>other_label] = other_label\n",
    "            w_svc,w_lg,w_nb = w\n",
    "            performance = getEvaluatingMatrix(val_y,val_preds)\n",
    "            fold_p.append(performance)\n",
    "        overall_p.append(fold_p)\n",
    "    overall_p = np.array(overall_p)\n",
    "    #overall_p\n",
    "    overall_p = np.mean(overall_p,axis=0)\n",
    "    ### create saving df\n",
    "    df_new = pd.DataFrame({\n",
    "                            'ngram':nr,\n",
    "                           'C_svc':C_svc,\n",
    "                           'C_lgs':C_lgs,\n",
    "                           'alpha_nb':alpha,\n",
    "                           'weight_svc':weight_list[:,0],\n",
    "                           'weight_lgs':weight_list[:,1],\n",
    "                           'weight_nb':weight_list[:,2],\n",
    "                           'accuracy_total':overall_p[:,0],\n",
    "                           'accuracy_01':overall_p[:,1],\n",
    "                           'label_0_precision':overall_p[:,2],\n",
    "                           'label_0_recall':overall_p[:,3],\n",
    "                           'label_0_count':overall_p[:,4],\n",
    "                           'label_1_precision':overall_p[:,5],\n",
    "                           'label_1_recall':overall_p[:,6],\n",
    "                           'label_1_count':overall_p[:,7],\n",
    "                           'label_2_precision':overall_p[:,8],\n",
    "                           'label_2_recall':overall_p[:,9],\n",
    "                           'label_2_count':overall_p[:,10]})\n",
    "    return df_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T21:10:21.913911Z",
     "start_time": "2018-09-11T21:10:21.899895Z"
    }
   },
   "outputs": [],
   "source": [
    "__file__ = '../GridSearch/'\n",
    "param_path = os.path.join(os.path.dirname(__file__), 'parameter/parameter_{}.csv')\n",
    "\n",
    "report_path = os.path.join(os.path.dirname(__file__), 'report/report_{}.csv')\n",
    "\n",
    "\n",
    "def parameter_generating(param_path, recreate=False):\n",
    "\n",
    "    try:\n",
    "        df_param = pd.read_csv(param_path)\n",
    "        if len(df_param) > 0 and not recreate:\n",
    "            print('{} params file already exist. no need to recreat!'.format(param_path))\n",
    "            return\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    print('create parameters for {}'.format(param_path))\n",
    "    C_svc_list = []\n",
    "    C_lgs_list = []\n",
    "    alpha_nb_list = []\n",
    "    nr_list = []\n",
    "\n",
    "    for C_s in np.concatenate([np.arange(0.01,1.2,0.1),np.arange(1.3,5,0.3)]):\n",
    "        for C_l in np.concatenate([np.arange(0.01,1.2,0.1),np.arange(1.3,5,0.3)]):\n",
    "            for ap in np.concatenate([np.arange(0.01,1.2,0.1),np.arange(1.3,5,0.3)]):\n",
    "                for nr in [1,2,3,4,5]:\n",
    "                    C_svc_list.append(C_s)\n",
    "                    C_lgs_list.append(C_l)\n",
    "                    alpha_nb_list.append(ap)\n",
    "                    nr_list.append(nr)\n",
    "    df_param = pd.DataFrame({'C_svc':C_svc_list,\n",
    "                             'C_lgs':C_lgs_list,\n",
    "                             'alpha_nb':alpha_nb_list,\n",
    "                             'ngram':nr_list})\n",
    "    df_param['trained'] = 'N'\n",
    "    df_param['indexing'] = df_param.index.values\n",
    "    df_param.to_csv(param_path,index=False)\n",
    "        \n",
    "def load_parameter(param_path):\n",
    "    df = pd.read_csv(param_path)\n",
    "    df_fil = df[df['trained'] == 'N'].copy()\n",
    "    if len(df_fil) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        df_fil = df_fil.sample(frac=1)\n",
    "        return df,df_fil.iloc[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T21:04:36.926864Z",
     "start_time": "2018-09-11T21:04:33.612127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create parameters for ../GridSearch/parameter/parameter_IDClassifier.csv\n",
      "create parameters for ../GridSearch/parameter/parameter_CutDebt.csv\n",
      "create parameters for ../GridSearch/parameter/parameter_WillingToPay.csv\n",
      "create parameters for ../GridSearch/parameter/parameter_IfKnowDebtor.csv\n",
      "create parameters for ../GridSearch/parameter/parameter_Installment.csv\n",
      "create parameters for ../GridSearch/parameter/parameter_ConfirmLoan.csv\n"
     ]
    }
   ],
   "source": [
    "param_path = os.path.join(os.path.dirname(__file__), 'parameter/parameter_{}.csv')\n",
    "for model in model_list:\n",
    "    evl_param_path = param_path.format(model)\n",
    "    parameter_generating(evl_param_path,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T21:29:18.667207Z",
     "start_time": "2018-09-11T21:29:09.946607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting phrase\n",
      "transform phrase\n",
      "finish training others\n",
      "../GridSearch/parameter/parameter_IDClassifier.csv params file already exist. no need to recreat!\n",
      "0\n",
      "fitting phrase\n",
      "transform phrase\n",
      "finish training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m../GridSearch/\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mdf_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdf_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_main\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC_svc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC_lgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha_nb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mdf_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_old\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdf_save\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevl_report_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m../GridSearch/\u001b[0m in \u001b[0;36mcv_run\u001b[0;34m(data_main, weight_list, nr, C_svc, C_lgs, alpha, nFold, seed)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mval_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mval_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ml_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mval_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mval_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_preds\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mother_label\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/Chatbot1.0/classifier/models/ml_models/ml.py\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self, sentence, lower_bounder, upper_bounder, debug)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mml_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mav_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds_ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/Chatbot1.0/classifier/models/ml_models/ml.py\u001b[0m in \u001b[0;36mpreds_ml\u001b[0;34m(self, sentence, removeTime)\u001b[0m\n\u001b[1;32m     93\u001b[0m         result = np.vstack((self.svc.predict_proba(matrix),\n\u001b[1;32m     94\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                                  self.nb.predict_proba(matrix)))\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mappear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_log_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict_log_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mappear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;31m# normalize by P(x) = P(f_1, ..., f_n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mlog_prob_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[1;32m    726\u001b[0m                 self.class_log_prior_)\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \"\"\"\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_multivector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;31m# csr_matvecs or csc_matvecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_matvecs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_path = os.path.join(os.path.dirname(__file__), 'parameter/parameter_{}.csv')\n",
    "report_path = os.path.join(os.path.dirname(__file__), 'report/report_{}.csv')\n",
    "\n",
    "posible_v = [0.1,1,3,5]\n",
    "weight_list = []\n",
    "for w_svc in posible_v:\n",
    "    for w_lg in posible_v:\n",
    "        for w_nb in posible_v:\n",
    "            weight_list.append([w_svc,w_lg,w_nb])\n",
    "# weight_list=[[5,1,1],[3,1,1]]\n",
    "weight_list = np.array(weight_list)\n",
    "\n",
    "model = 'IDClassifier'\n",
    "evl_param_path = param_path.format(model)\n",
    "evl_report_path = report_path.format(model)\n",
    "data_main,data_other = get_mainVSothers_data(model)\n",
    "other_model = train_other_model(data_other,ng_range=(1,3),C_svc=1,C_lgs=1,alpha_nb=1)\n",
    "other_label = data_main['label'].max()\n",
    "\n",
    "parameter_generating(evl_param_path,False)\n",
    "\n",
    "while True:\n",
    "    loads = load_parameter(evl_param_path)\n",
    "    if loads is None:\n",
    "        print('ending search')\n",
    "        break\n",
    "    df_param,cur_params = loads\n",
    "    C_svc = cur_params['C_svc']\n",
    "    C_lgs = cur_params['C_lgs']\n",
    "    alpha_nb = cur_params['alpha_nb']\n",
    "    nr = cur_params['ngram']\n",
    "    indexing = cur_params['indexing']\n",
    "    df_param.loc[df_param.indexing==indexing,'trained'] = 'Y'\n",
    "    \n",
    "    #### load old report\n",
    "    try:\n",
    "        df_old = pd.read_csv(evl_report_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        df_old = pd.DataFrame()\n",
    "    df_new = cv_run(data_main,weight_list,nr,C_svc,C_lgs,alpha_nb)\n",
    "    df_save = pd.concat([df_old,df_new],ignore_index=True)\n",
    "    df_save.to_csv(evl_report_path,index=False)\n",
    "    df_param.to_csv(evl_param_path,index=False)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "                         \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T18:01:27.264155Z",
     "start_time": "2018-09-11T18:01:27.255091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9338146811070999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#1. accuracy\n",
    "accuracy = accuracy_score(val_y,val_preds)\n",
    "print(accuracy)\n",
    "\n",
    "#2. precision\n",
    "cm = confusion_matrix(val_y,val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T18:26:01.332515Z",
     "start_time": "2018-09-11T18:26:01.323829Z"
    }
   },
   "outputs": [],
   "source": [
    "def getEvaluatingMatrix(val_y,val_preds):\n",
    "    cm = confusion_matrix(val_y,val_preds)\n",
    "    accuracy_total = (cm[0,0]+cm[1,1]+cm[2,2]) / np.sum(cm)\n",
    "\n",
    "    accuracy_01 = (cm[0,0]+cm[1,1]) / np.sum(cm[:2,:])\n",
    "\n",
    "    ###\n",
    "    label_0_precision = cm[0,0] / np.sum(cm[:,0])\n",
    "    label_0_recall = cm[0,0] / np.sum(cm[0,:])\n",
    "    label_0_count = np.sum(cm[0,:])\n",
    "    label_1_precision = cm[1,1] / np.sum(cm[:,1])\n",
    "    label_1_recall = cm[1,1] / np.sum(cm[1,:])\n",
    "    label_1_count = np.sum(cm[1,:])\n",
    "    label_2_precision = cm[2,2] / np.sum(cm[:,2])\n",
    "    label_2_recall = cm[2,2] / np.sum(cm[2,:])\n",
    "    label_2_count = np.sum(cm[2,:])\n",
    "    return (accuracy_total,\n",
    "            accuracy_01,\n",
    "            label_0_precision,\n",
    "            label_0_recall,\n",
    "            label_0_count,\n",
    "            label_1_precision,\n",
    "            label_1_recall,\n",
    "            label_1_count,\n",
    "            label_2_precision,\n",
    "            label_2_recall,\n",
    "            label_2_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T18:26:01.760304Z",
     "start_time": "2018-09-11T18:26:01.738527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9338146811070999,\n",
       " 0.72,\n",
       " 0.9090909090909091,\n",
       " 0.7352941176470589,\n",
       " 68,\n",
       " 0.9620253164556962,\n",
       " 0.7102803738317757,\n",
       " 107,\n",
       " 0.9325681492109039,\n",
       " 0.9908536585365854,\n",
       " 656)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getEvaluatingMatrix(val_y,val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T18:09:02.447151Z",
     "start_time": "2018-09-11T18:09:02.438797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 50,   0,  18],\n",
       "       [  2,  76,  29],\n",
       "       [  3,   3, 650]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T18:06:32.085727Z",
     "start_time": "2018-09-11T18:06:32.077914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T19:02:27.236188Z",
     "start_time": "2018-09-11T19:02:27.230579Z"
    }
   },
   "outputs": [],
   "source": [
    "test=[[(1,2,3),(2,3,4)],[(2,4,6),(3,6,8)]]\n",
    "test = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T19:02:38.781597Z",
     "start_time": "2018-09-11T19:02:38.769935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T19:34:47.627210Z",
     "start_time": "2018-09-11T19:34:47.609473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  age  address\n",
       "0     1    2        1\n",
       "1     1    3        2\n",
       "2     1    4        3"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'name':1,'age':[2,3,4],'address':[1,2,3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T20:12:10.584581Z",
     "start_time": "2018-09-11T20:12:10.567929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.11, 0.21, 0.31, 0.41, 0.51, 0.61, 0.71, 0.81, 0.91, 1.01,\n",
       "       1.11, 1.21, 1.31, 1.41, 1.51, 1.61, 1.71, 1.81, 1.91, 2.01, 2.11,\n",
       "       2.21, 2.31, 2.41, 2.51, 2.61, 2.71, 2.81, 2.91])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0.01,3,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
