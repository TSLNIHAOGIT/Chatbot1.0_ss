{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:44.154410Z",
     "start_time": "2018-06-13T14:05:43.854276Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:45.045842Z",
     "start_time": "2018-06-13T14:05:45.021036Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1575, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>split_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>啊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>你   有病   吧   我   跟 你   说 过   很 多次   了   我   不是...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>不是   什么 事</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>啊   对</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>对</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>对</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>我   不是   啊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>噢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>啊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>啊</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                         split_text\n",
       "0      0                                                  啊\n",
       "1      1  你   有病   吧   我   跟 你   说 过   很 多次   了   我   不是...\n",
       "2      1                                          不是   什么 事\n",
       "3      0                                              啊   对\n",
       "4      0                                                  对\n",
       "5      0                                                  对\n",
       "6      1                                         我   不是   啊\n",
       "7      0                                                  噢\n",
       "8      0                                                  啊\n",
       "9      0                                                  啊"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path = '/home/kai/data/jiangning/Chatbot_1/Chatbot1.0/data/'\n",
    "path = '../../data/IDClassifier/'\n",
    "data = pd.read_csv(path + 'cleaned_mock_up_data.csv', encoding='utf8')\n",
    "print(data.shape)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:45.205574Z",
     "start_time": "2018-06-13T14:05:45.200417Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# phrase_vectorizer1 = TfidfVectorizer(ngram_range=(1,3),\n",
    "#                                     strip_accents='unicode', \n",
    "#                                     max_features=100000, \n",
    "#                                     analyzer='word',\n",
    "#                                     sublinear_tf=True,\n",
    "#                                     token_pattern=r'\\w{1,}')\n",
    "# data.split_text.iloc[:2]\n",
    "# dd = ['I like apple do you like it', 'I do not like banana']\n",
    "# phrase_vectorizer1.fit(dd)\n",
    "\n",
    "# print('transform phrase')\n",
    "# phrase = phrase_vectorizer1.transform(dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:45.369589Z",
     "start_time": "2018-06-13T14:05:45.364674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你   有病   吧   我   跟 你   说 过   很 多次   了   我   不是   他'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.split_text.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:45.562635Z",
     "start_time": "2018-06-13T14:05:45.559839Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# phrase_vectorizer1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:45.786849Z",
     "start_time": "2018-06-13T14:05:45.733306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting phrase\n",
      "transform phrase\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1575x3941 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 15419 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                    strip_accents='unicode', \n",
    "                                    max_features=100000, \n",
    "                                    analyzer='word',\n",
    "                                    sublinear_tf=True,\n",
    "                                    token_pattern=r'\\w{1,}')\n",
    "\n",
    "print('fitting phrase')\n",
    "phrase_vectorizer.fit(data.split_text)\n",
    "\n",
    "print('transform phrase')\n",
    "phrase = phrase_vectorizer.transform(data.split_text)\n",
    "\n",
    "phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:46.148203Z",
     "start_time": "2018-06-13T14:05:46.024738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.969523809524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "l_svc = LinearSVC()\n",
    "clf = CalibratedClassifierCV(l_svc) \n",
    "clf.fit(phrase, data.label)\n",
    "print(clf.score(phrase, data.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:46.182091Z",
     "start_time": "2018-06-13T14:05:46.174830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ..., 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(phrase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:46.473340Z",
     "start_time": "2018-06-13T14:05:46.470122Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:46.668002Z",
     "start_time": "2018-06-13T14:05:46.643149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.930158730159\n"
     ]
    }
   ],
   "source": [
    "log_r = LogisticRegression()\n",
    "log_r.fit(phrase, data.label)\n",
    "print(log_r.score(phrase, data.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:46.807982Z",
     "start_time": "2018-06-13T14:05:46.803472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ..., 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "print(log_r.predict(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:46.966176Z",
     "start_time": "2018-06-13T14:05:46.963335Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# l_svc.predict(phrase)-log_r.predict(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:47.313076Z",
     "start_time": "2018-06-13T14:05:47.300025Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:47.493603Z",
     "start_time": "2018-06-13T14:05:47.487915Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data.label.values)\n",
    "onelabels = le.transform(data.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:47.672965Z",
     "start_time": "2018-06-13T14:05:47.668334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onelabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:47.851939Z",
     "start_time": "2018-06-13T14:05:47.846182Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "multicoder = MultiLabelBinarizer()\n",
    "lables = multicoder.fit_transform([data.label.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:48.031487Z",
     "start_time": "2018-06-13T14:05:48.026879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:48.248985Z",
     "start_time": "2018-06-13T14:05:48.244115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 2, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:48.498607Z",
     "start_time": "2018-06-13T14:05:48.495881Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lables.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:49.286978Z",
     "start_time": "2018-06-13T14:05:49.051744Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/kai/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:104: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.16381\n",
      "[10]\tvalid_0's multi_error: 0.144127\n",
      "[15]\tvalid_0's multi_error: 0.12381\n",
      "[20]\tvalid_0's multi_error: 0.111746\n",
      "[25]\tvalid_0's multi_error: 0.0977778\n",
      "[30]\tvalid_0's multi_error: 0.0895238\n",
      "[35]\tvalid_0's multi_error: 0.0888889\n",
      "[40]\tvalid_0's multi_error: 0.0844444\n",
      "[45]\tvalid_0's multi_error: 0.0825397\n",
      "[50]\tvalid_0's multi_error: 0.0806349\n",
      "[55]\tvalid_0's multi_error: 0.0774603\n",
      "[60]\tvalid_0's multi_error: 0.0768254\n",
      "[65]\tvalid_0's multi_error: 0.0761905\n",
      "[70]\tvalid_0's multi_error: 0.0749206\n",
      "[75]\tvalid_0's multi_error: 0.0749206\n",
      "[80]\tvalid_0's multi_error: 0.0736508\n",
      "[85]\tvalid_0's multi_error: 0.072381\n",
      "[90]\tvalid_0's multi_error: 0.072381\n",
      "[95]\tvalid_0's multi_error: 0.072381\n",
      "[100]\tvalid_0's multi_error: 0.071746\n",
      "[105]\tvalid_0's multi_error: 0.0692063\n",
      "[110]\tvalid_0's multi_error: 0.0692063\n",
      "[115]\tvalid_0's multi_error: 0.0692063\n",
      "[120]\tvalid_0's multi_error: 0.0692063\n",
      "[125]\tvalid_0's multi_error: 0.0692063\n",
      "[130]\tvalid_0's multi_error: 0.0692063\n",
      "[135]\tvalid_0's multi_error: 0.0692063\n",
      "[140]\tvalid_0's multi_error: 0.0692063\n",
      "[145]\tvalid_0's multi_error: 0.0692063\n",
      "[150]\tvalid_0's multi_error: 0.0692063\n",
      "[155]\tvalid_0's multi_error: 0.0692063\n",
      "[160]\tvalid_0's multi_error: 0.0692063\n",
      "[165]\tvalid_0's multi_error: 0.0692063\n",
      "[170]\tvalid_0's multi_error: 0.0692063\n",
      "[175]\tvalid_0's multi_error: 0.0692063\n",
      "[180]\tvalid_0's multi_error: 0.0692063\n",
      "[185]\tvalid_0's multi_error: 0.0692063\n",
      "[190]\tvalid_0's multi_error: 0.0692063\n",
      "[195]\tvalid_0's multi_error: 0.0692063\n",
      "[200]\tvalid_0's multi_error: 0.0692063\n",
      "[205]\tvalid_0's multi_error: 0.0692063\n",
      "[210]\tvalid_0's multi_error: 0.0692063\n",
      "[215]\tvalid_0's multi_error: 0.0692063\n",
      "[220]\tvalid_0's multi_error: 0.0692063\n",
      "[225]\tvalid_0's multi_error: 0.0692063\n",
      "[230]\tvalid_0's multi_error: 0.0692063\n",
      "[235]\tvalid_0's multi_error: 0.0692063\n",
      "[240]\tvalid_0's multi_error: 0.0692063\n",
      "[245]\tvalid_0's multi_error: 0.0692063\n",
      "[250]\tvalid_0's multi_error: 0.0692063\n",
      "[255]\tvalid_0's multi_error: 0.0692063\n",
      "[260]\tvalid_0's multi_error: 0.0692063\n",
      "[265]\tvalid_0's multi_error: 0.0692063\n",
      "[270]\tvalid_0's multi_error: 0.0692063\n",
      "[275]\tvalid_0's multi_error: 0.0692063\n",
      "[280]\tvalid_0's multi_error: 0.0692063\n",
      "[285]\tvalid_0's multi_error: 0.0692063\n",
      "[290]\tvalid_0's multi_error: 0.0692063\n",
      "[295]\tvalid_0's multi_error: 0.0692063\n",
      "[300]\tvalid_0's multi_error: 0.0692063\n",
      "[305]\tvalid_0's multi_error: 0.0692063\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid_0's multi_error: 0.0692063\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.2,\n",
    "    'num_iterations':1000,\n",
    "    'application': 'multiclassova',\n",
    "    'num_class': 3,\n",
    "    'num_leaves': 31,\n",
    "    'verbosity': -1,\n",
    "    'metric': 'multi_error',\n",
    "    'data_random_seed': 2,\n",
    "#     'bagging_fraction': 0.8,\n",
    "#     'feature_fraction': 0.6,\n",
    "    'nthread': 4,\n",
    "    'lambda_l1': 1,\n",
    "    'lambda_l2': 1,\n",
    "    'early_stopping_round':200\n",
    "} \n",
    "\n",
    "# lgbm_train = lgb.Dataset(phrase, data.label)\n",
    "lgbm_train = lgb.Dataset(phrase, onelabels)\n",
    "lgbm_val = lgb.Dataset(phrase, onelabels)\n",
    "lgbm_model = lgb.train(params,lgbm_train,valid_sets=lgbm_val, verbose_eval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:49.826413Z",
     "start_time": "2018-06-13T14:05:49.804015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.90568070e-01   4.83210937e-03   9.34213248e-03]\n",
      " [  5.35080526e-04   9.98512677e-01   2.50031682e-03]\n",
      " [  2.19988812e-02   9.84001859e-01   9.91147251e-03]\n",
      " ..., \n",
      " [  6.63365535e-03   9.91822966e-01   3.98196403e-03]\n",
      " [  6.47736122e-01   1.14001591e-01   1.80868007e-01]\n",
      " [  2.40402187e-01   5.75542830e-03   7.78350920e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(lgbm_model.predict(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:51.864650Z",
     "start_time": "2018-06-13T14:05:51.861851Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lgbm_model.predict(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:52.205430Z",
     "start_time": "2018-06-13T14:05:52.200937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1575x3941 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 15419 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:57.146586Z",
     "start_time": "2018-06-13T14:05:57.107447Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save tfidf\n",
    "pickle.dump(phrase_vectorizer, open(\"../../savedModel/IDClassifier/tfidf.pickle\", \"wb\"))\n",
    "# pickle.dump(train_comment_features, open(\"train_comment_features.pickle\", \"wb\"))\n",
    "# pickle.dump(test_comment_features, open(\"test_comment_features.pickle\", \"wb\"))\n",
    "\n",
    "# save linear svc\n",
    "pickle.dump(clf, open(\"../../savedModel/IDClassifier/LinearSVC.pickle\", \"wb\"))\n",
    "# save logistic\n",
    "pickle.dump(log_r, open(\"../../savedModel/IDClassifier/Logistic.pickle\", \"wb\"))\n",
    "# save lightGBM\n",
    "pickle.dump(lgbm_model, open(\"../../savedModel/IDClassifier/Lgbm.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:05:57.694004Z",
     "start_time": "2018-06-13T14:05:57.691189Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.load(open(\"../../savedModel/IDClassifier/Lgbm.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:06:12.971102Z",
     "start_time": "2018-06-13T14:06:12.966589Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = phrase_vectorizer.transform(['是 呀'])\n",
    "# test = phrase_vectorizer.transform(['我 在 洗澡'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:06:13.144840Z",
     "start_time": "2018-06-13T14:06:13.140050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3941 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:06:13.313098Z",
     "start_time": "2018-06-13T14:06:13.306429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.90866398,  0.03529406,  0.05604196]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(test) # linear svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:06:13.472558Z",
     "start_time": "2018-06-13T14:06:13.467353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74566077,  0.14996941,  0.10436982]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_r.predict_proba(test) # logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:06:13.655761Z",
     "start_time": "2018-06-13T14:06:13.650495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97235174,  0.0247113 ,  0.01844675]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_model.predict(test) # light gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T14:06:14.292814Z",
     "start_time": "2018-06-13T14:06:14.272542Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label= 0\n",
      "prob= 0.972351739991\n"
     ]
    }
   ],
   "source": [
    "# basic logic: find the max probability of 3 models, if it is larger than threshold, return the corresponding label, otherwise, return 2 (others).\n",
    "result = np.vstack((clf.predict_proba(test),log_r.predict_proba(test),lgbm_model.predict(test)))\n",
    "pos = np.where(result == np.max(result))\n",
    "\n",
    "threshold = 0.7\n",
    "if np.max(result)<threshold:\n",
    "    label = 2\n",
    "else:\n",
    "    label = pos[1]\n",
    "    label = label[0]\n",
    "    \n",
    "print('label=',label)\n",
    "print('prob=',np.max(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the whole thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T15:28:26.722932Z",
     "start_time": "2018-06-13T15:28:26.693802Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import jieba\n",
    "# import numpy as np\n",
    "\n",
    "# class IDClassifier:\n",
    "    \n",
    "#     def __init__(self, **model):\n",
    "#         \"\"\"\n",
    "#         suggested parameters:\n",
    "#         svc, logistic, lightgbm, jieba_path,tfidf\n",
    "#         \"\"\"\n",
    "#         self._load_model(**model)\n",
    "        \n",
    "#     def _load_model(self,**model):\n",
    "#         self.svc = model.get('svc')\n",
    "#         self.logistic = model.get('logistic')\n",
    "#         self.lightgbm = model.get('lightgbm')\n",
    "#         self.tfidf = model.get('tfidf')\n",
    "#         # load jieba\n",
    "#         jieba_path = model.get('jieba_path')\n",
    "#         if jieba_path is not None:\n",
    "#             jieba.load_userdict(jieba_path)\n",
    "        \n",
    "        \n",
    "#     def classify(self, sentence):\n",
    "#         sentence = jieba.cut(sentence, cut_all = False)\n",
    "#         sentence = ' '.join(sentence)\n",
    "#         matrix = self.tfidf.transform([sentence])\n",
    "#         result = np.vstack((self.svc.predict_proba(matrix),\n",
    "#                             self.logistic.predict_proba(matrix),\n",
    "#                             self.lightgbm.predict(matrix)))\n",
    "#         max_pred = np.max(result, axis=0)\n",
    "#         max_arg = np.argmax(max_pred)\n",
    "#         threshold = 0.6\n",
    "#         if np.max(max_pred<threshold):\n",
    "#             label = 2\n",
    "#         else:\n",
    "#             label = max_arg\n",
    "#         return (label, np.max(max_pred))\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T15:28:27.867588Z",
     "start_time": "2018-06-13T15:28:27.844683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0.95201366423625655)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idc.classify('我在洗澡')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T15:28:28.496016Z",
     "start_time": "2018-06-13T15:28:28.474840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T15:44:28.800113Z",
     "start_time": "2018-06-13T15:44:28.755225Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IDClassifier_py import IDClassifier\n",
    "idc = IDClassifier(svc=clf, logistic=log_r, lightgbm=lgbm_model, tfidf=phrase_vectorizer, jieba_path='../WordCut/userdict.txt')\n",
    "\n",
    "pickle.dump(idc, open(\"../../savedModel/IDClassifier/IDClassifier.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T15:36:35.906115Z",
     "start_time": "2018-06-13T15:36:35.883943Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## when apply, need to import sys path of the .py file, then load pkl file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
