{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:13.746432Z",
     "start_time": "2018-06-13T19:09:13.724540Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:14.584829Z",
     "start_time": "2018-06-13T19:09:14.553880Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1435, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>split_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>喜喜</td>\n",
       "      <td>喂唉你好</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>我 知道 你 来 要钱   我 没 钱</td>\n",
       "      <td>知道你是来催钱的，老子没钱</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>什么</td>\n",
       "      <td>啊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>你 收集 公司 的 工作人员 吗 ？</td>\n",
       "      <td>你是催收公司的工作人员么</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>我 是 李 大宝 不是 张 的 狗 。</td>\n",
       "      <td>我是李大宝，不是张二狗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>谁</td>\n",
       "      <td>谁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>我 不是 你 想要 的</td>\n",
       "      <td>我不是你要找的人</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>好</td>\n",
       "      <td>嗯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>你 重复 了 它   你 没有 听到 它 。</td>\n",
       "      <td>你重新说一下，刚才没听清</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>唉呀</td>\n",
       "      <td>唉呀</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label              split_text           text\n",
       "0      0                      喜喜           喂唉你好\n",
       "1      0     我 知道 你 来 要钱   我 没 钱  知道你是来催钱的，老子没钱\n",
       "2      0                      什么              啊\n",
       "3      0      你 收集 公司 的 工作人员 吗 ？   你是催收公司的工作人员么\n",
       "4      1     我 是 李 大宝 不是 张 的 狗 。    我是李大宝，不是张二狗\n",
       "5      2                       谁              谁\n",
       "6      1             我 不是 你 想要 的       我不是你要找的人\n",
       "7      0                       好              嗯\n",
       "8      2  你 重复 了 它   你 没有 听到 它 。   你重新说一下，刚才没听清\n",
       "9      0                      唉呀             唉呀"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path = '/home/kai/data/jiangning/Chatbot_1/Chatbot1.0/data/'\n",
    "path = '../../data/IDClassifier/'\n",
    "data = pd.read_csv(path + 'cleaned_mock_up_data.csv', encoding='utf8')\n",
    "print(data.shape)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:15.272548Z",
     "start_time": "2018-06-13T19:09:15.251578Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# phrase_vectorizer1 = TfidfVectorizer(ngram_range=(1,3),\n",
    "#                                     strip_accents='unicode', \n",
    "#                                     max_features=100000, \n",
    "#                                     analyzer='word',\n",
    "#                                     sublinear_tf=True,\n",
    "#                                     token_pattern=r'\\w{1,}')\n",
    "# data.split_text.iloc[:2]\n",
    "# dd = ['I like apple do you like it', 'I do not like banana']\n",
    "# phrase_vectorizer1.fit(dd)\n",
    "\n",
    "# print('transform phrase')\n",
    "# phrase = phrase_vectorizer1.transform(dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:21.646917Z",
     "start_time": "2018-06-13T19:09:21.624458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我 知道 你 来 要钱   我 没 钱'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.split_text.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:22.521582Z",
     "start_time": "2018-06-13T19:09:22.500034Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# phrase_vectorizer1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:23.292904Z",
     "start_time": "2018-06-13T19:09:23.236402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting phrase\n",
      "transform phrase\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1435x3789 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14364 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                    strip_accents='unicode', \n",
    "                                    max_features=100000, \n",
    "                                    analyzer='word',\n",
    "                                    sublinear_tf=True,\n",
    "                                    token_pattern=r'\\w{1,}')\n",
    "\n",
    "print('fitting phrase')\n",
    "phrase_vectorizer.fit(data.split_text)\n",
    "\n",
    "print('transform phrase')\n",
    "phrase = phrase_vectorizer.transform(data.split_text)\n",
    "\n",
    "phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:26.437436Z",
     "start_time": "2018-06-13T19:09:26.354384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.970034843206\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "l_svc = LinearSVC()\n",
    "clf = CalibratedClassifierCV(l_svc) \n",
    "clf.fit(phrase, data.label)\n",
    "print(clf.score(phrase, data.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:26.968154Z",
     "start_time": "2018-06-13T19:09:26.945001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(phrase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:28.111058Z",
     "start_time": "2018-06-13T19:09:28.089594Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:28.364902Z",
     "start_time": "2018-06-13T19:09:28.330409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926132404181\n"
     ]
    }
   ],
   "source": [
    "log_r = LogisticRegression()\n",
    "log_r.fit(phrase, data.label)\n",
    "print(log_r.score(phrase, data.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:28.888462Z",
     "start_time": "2018-06-13T19:09:28.865214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(log_r.predict(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:29.384752Z",
     "start_time": "2018-06-13T19:09:29.364114Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# l_svc.predict(phrase)-log_r.predict(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:30.007368Z",
     "start_time": "2018-06-13T19:09:29.986355Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:30.247883Z",
     "start_time": "2018-06-13T19:09:30.226509Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data.label.values)\n",
    "onelabels = le.transform(data.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:30.495785Z",
     "start_time": "2018-06-13T19:09:30.473894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onelabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:30.747932Z",
     "start_time": "2018-06-13T19:09:30.725804Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "multicoder = MultiLabelBinarizer()\n",
    "lables = multicoder.fit_transform([data.label.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:31.009610Z",
     "start_time": "2018-06-13T19:09:30.986527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:31.438842Z",
     "start_time": "2018-06-13T19:09:31.416700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:32.204907Z",
     "start_time": "2018-06-13T19:09:32.183044Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lables.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:34.421922Z",
     "start_time": "2018-06-13T19:09:34.198291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[5]\tvalid_0's multi_error: 0.148432\n",
      "[10]\tvalid_0's multi_error: 0.12892\n",
      "[15]\tvalid_0's multi_error: 0.110801\n",
      "[20]\tvalid_0's multi_error: 0.100348\n",
      "[25]\tvalid_0's multi_error: 0.0885017\n",
      "[30]\tvalid_0's multi_error: 0.0815331\n",
      "[35]\tvalid_0's multi_error: 0.0808362\n",
      "[40]\tvalid_0's multi_error: 0.0745645\n",
      "[45]\tvalid_0's multi_error: 0.0738676\n",
      "[50]\tvalid_0's multi_error: 0.0731707\n",
      "[55]\tvalid_0's multi_error: 0.0724739\n",
      "[60]\tvalid_0's multi_error: 0.0710801\n",
      "[65]\tvalid_0's multi_error: 0.0675958\n",
      "[70]\tvalid_0's multi_error: 0.0675958\n",
      "[75]\tvalid_0's multi_error: 0.066899\n",
      "[80]\tvalid_0's multi_error: 0.0655052\n",
      "[85]\tvalid_0's multi_error: 0.0648084\n",
      "[90]\tvalid_0's multi_error: 0.0634146\n",
      "[95]\tvalid_0's multi_error: 0.0634146\n",
      "[100]\tvalid_0's multi_error: 0.0634146\n",
      "[105]\tvalid_0's multi_error: 0.0634146\n",
      "[110]\tvalid_0's multi_error: 0.0620209\n",
      "[115]\tvalid_0's multi_error: 0.061324\n",
      "[120]\tvalid_0's multi_error: 0.0627178\n",
      "[125]\tvalid_0's multi_error: 0.061324\n",
      "[130]\tvalid_0's multi_error: 0.061324\n",
      "[135]\tvalid_0's multi_error: 0.061324\n",
      "[140]\tvalid_0's multi_error: 0.061324\n",
      "[145]\tvalid_0's multi_error: 0.061324\n",
      "[150]\tvalid_0's multi_error: 0.061324\n",
      "[155]\tvalid_0's multi_error: 0.061324\n",
      "[160]\tvalid_0's multi_error: 0.061324\n",
      "[165]\tvalid_0's multi_error: 0.061324\n",
      "[170]\tvalid_0's multi_error: 0.061324\n",
      "[175]\tvalid_0's multi_error: 0.061324\n",
      "[180]\tvalid_0's multi_error: 0.061324\n",
      "[185]\tvalid_0's multi_error: 0.061324\n",
      "[190]\tvalid_0's multi_error: 0.061324\n",
      "[195]\tvalid_0's multi_error: 0.061324\n",
      "[200]\tvalid_0's multi_error: 0.061324\n",
      "[205]\tvalid_0's multi_error: 0.061324\n",
      "[210]\tvalid_0's multi_error: 0.061324\n",
      "[215]\tvalid_0's multi_error: 0.061324\n",
      "[220]\tvalid_0's multi_error: 0.061324\n",
      "[225]\tvalid_0's multi_error: 0.061324\n",
      "[230]\tvalid_0's multi_error: 0.061324\n",
      "[235]\tvalid_0's multi_error: 0.061324\n",
      "[240]\tvalid_0's multi_error: 0.061324\n",
      "[245]\tvalid_0's multi_error: 0.061324\n",
      "[250]\tvalid_0's multi_error: 0.061324\n",
      "[255]\tvalid_0's multi_error: 0.061324\n",
      "[260]\tvalid_0's multi_error: 0.061324\n",
      "[265]\tvalid_0's multi_error: 0.061324\n",
      "[270]\tvalid_0's multi_error: 0.061324\n",
      "[275]\tvalid_0's multi_error: 0.061324\n",
      "[280]\tvalid_0's multi_error: 0.061324\n",
      "[285]\tvalid_0's multi_error: 0.061324\n",
      "[290]\tvalid_0's multi_error: 0.061324\n",
      "[295]\tvalid_0's multi_error: 0.061324\n",
      "[300]\tvalid_0's multi_error: 0.061324\n",
      "[305]\tvalid_0's multi_error: 0.061324\n",
      "[310]\tvalid_0's multi_error: 0.061324\n",
      "[315]\tvalid_0's multi_error: 0.061324\n",
      "Early stopping, best iteration is:\n",
      "[115]\tvalid_0's multi_error: 0.061324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/kai/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:104: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.2,\n",
    "    'num_iterations':1000,\n",
    "    'application': 'multiclassova',\n",
    "    'num_class': 3,\n",
    "    'num_leaves': 31,\n",
    "    'verbosity': -1,\n",
    "    'metric': 'multi_error',\n",
    "    'data_random_seed': 2,\n",
    "#     'bagging_fraction': 0.8,\n",
    "#     'feature_fraction': 0.6,\n",
    "    'nthread': 4,\n",
    "    'lambda_l1': 1,\n",
    "    'lambda_l2': 1,\n",
    "    'early_stopping_round':200\n",
    "} \n",
    "\n",
    "# lgbm_train = lgb.Dataset(phrase, data.label)\n",
    "lgbm_train = lgb.Dataset(phrase, onelabels)\n",
    "lgbm_val = lgb.Dataset(phrase, onelabels)\n",
    "lgbm_model = lgb.train(params,lgbm_train,valid_sets=lgbm_val, verbose_eval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:35.315502Z",
     "start_time": "2018-06-13T19:09:35.278276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.73547642  0.04426599  0.18111404]\n",
      " [ 0.98311386  0.01027268  0.00852655]\n",
      " [ 0.61446525  0.03224424  0.36991184]\n",
      " ..., \n",
      " [ 0.73547642  0.04426599  0.18111404]\n",
      " [ 0.99648274  0.00323028  0.00292047]\n",
      " [ 0.72330022  0.18173076  0.0093197 ]]\n"
     ]
    }
   ],
   "source": [
    "print(lgbm_model.predict(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:36.020795Z",
     "start_time": "2018-06-13T19:09:36.000572Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lgbm_model.predict(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:36.411940Z",
     "start_time": "2018-06-13T19:09:36.391048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1435x3789 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14364 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:38.935270Z",
     "start_time": "2018-06-13T19:09:38.888358Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save tfidf\n",
    "pickle.dump(phrase_vectorizer, open(\"../../savedModel/IDClassifier/tfidf.pickle\", \"wb\"))\n",
    "# pickle.dump(train_comment_features, open(\"train_comment_features.pickle\", \"wb\"))\n",
    "# pickle.dump(test_comment_features, open(\"test_comment_features.pickle\", \"wb\"))\n",
    "\n",
    "# save linear svc\n",
    "pickle.dump(clf, open(\"../../savedModel/IDClassifier/LinearSVC.pickle\", \"wb\"))\n",
    "# save logistic\n",
    "pickle.dump(log_r, open(\"../../savedModel/IDClassifier/Logistic.pickle\", \"wb\"))\n",
    "# save lightGBM\n",
    "pickle.dump(lgbm_model, open(\"../../savedModel/IDClassifier/Lgbm.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:40.037408Z",
     "start_time": "2018-06-13T19:09:40.016935Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.load(open(\"../../savedModel/IDClassifier/Lgbm.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:41.177786Z",
     "start_time": "2018-06-13T19:09:41.154848Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "sentence = '对啊'\n",
    "sentence = jieba.cut(sentence, cut_all = False)\n",
    "sentence = ' '.join(sentence)\n",
    "test = phrase_vectorizer.transform([sentence])\n",
    "# test = phrase_vectorizer.transform(['我 在 洗澡'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:42.136606Z",
     "start_time": "2018-06-13T19:09:42.113743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3789 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:44.801893Z",
     "start_time": "2018-06-13T19:09:44.779104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97165868,  0.00946117,  0.01888016]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(test) # linear svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:46.452473Z",
     "start_time": "2018-06-13T19:09:46.430267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.87116412,  0.07289443,  0.05594145]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_r.predict_proba(test) # logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:47.903153Z",
     "start_time": "2018-06-13T19:09:47.881342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99303539,  0.00546324,  0.00490804]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_model.predict(test) # light gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:09:49.995160Z",
     "start_time": "2018-06-13T19:09:49.963360Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label= 0\n",
      "prob= 0.993035390427\n"
     ]
    }
   ],
   "source": [
    "# basic logic: find the max probability of 3 models, if it is larger than threshold, return the corresponding label, otherwise, return 2 (others).\n",
    "result = np.vstack((clf.predict_proba(test),log_r.predict_proba(test),lgbm_model.predict(test)))\n",
    "pos = np.where(result == np.max(result))\n",
    "\n",
    "threshold = 0.7\n",
    "if np.max(result)<threshold:\n",
    "    label = 2\n",
    "else:\n",
    "    label = pos[1]\n",
    "    label = label[0]\n",
    "    \n",
    "print('label=',label)\n",
    "print('prob=',np.max(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the whole thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:13:39.566007Z",
     "start_time": "2018-06-13T19:13:39.517312Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import jieba\n",
    "# import numpy as np\n",
    "\n",
    "# class IDClassifier1:\n",
    "    \n",
    "#     def __init__(self, **model):\n",
    "#         \"\"\"\n",
    "#         suggested parameters:\n",
    "#         svc, logistic, lightgbm, jieba_path,tfidf\n",
    "#         \"\"\"\n",
    "#         self._load_model(**model)\n",
    "        \n",
    "#     def _load_model(self,**model):\n",
    "#         self.svc = model.get('svc')\n",
    "#         self.logistic = model.get('logistic')\n",
    "#         self.lightgbm = model.get('lightgbm')\n",
    "#         self.tfidf = model.get('tfidf')\n",
    "#         # load jieba\n",
    "#         jieba_path = model.get('jieba_path')\n",
    "#         if jieba_path is not None:\n",
    "#             jieba.load_userdict(jieba_path)\n",
    "        \n",
    "        \n",
    "#     def classify(self, sentence):\n",
    "#         sentence = jieba.cut(sentence, cut_all = False)\n",
    "#         sentence = ' '.join(sentence)\n",
    "#         matrix = self.tfidf.transform([sentence])\n",
    "#         result = np.vstack((self.svc.predict_proba(matrix),\n",
    "#                             self.logistic.predict_proba(matrix),\n",
    "#                             self.lightgbm.predict(matrix)))\n",
    "#         max_pred = np.max(result, axis=0)\n",
    "#         max_arg = np.argmax(max_pred)\n",
    "#         threshold = 0.6\n",
    "#         if np.max(max_pred)<threshold:\n",
    "#             label = 2\n",
    "#             print(max_pred)\n",
    "#         else:\n",
    "#             label = max_arg\n",
    "#         return (label, np.max(max_pred))\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:13:49.241068Z",
     "start_time": "2018-06-13T19:13:49.218027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:13:49.965703Z",
     "start_time": "2018-06-13T19:13:49.923207Z"
    }
   },
   "outputs": [],
   "source": [
    "from IDClassifier_py import IDClassifier\n",
    "idc = IDClassifier(svc=clf, logistic=log_r, lightgbm=lgbm_model, tfidf=phrase_vectorizer, jieba_path='../WordCut/userdict.txt')\n",
    "\n",
    "pickle.dump(idc, open(\"../../savedModel/IDClassifier/IDClassifier.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T19:13:51.779874Z",
     "start_time": "2018-06-13T19:13:51.758760Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## when apply, need to import sys path of the .py file, then load pkl file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
