{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T20:03:26.306952Z",
     "start_time": "2018-06-26T20:03:25.340638Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.444 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#encoding=utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.translate import NotTranslated\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "\n",
    "import jieba\n",
    "jieba.load_userdict(\"../WordCut/userdict.txt\")\n",
    "\n",
    "import gc\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "model_list = ['CutDebt','IDClassifier','IfKnowDebtor','Installment','WillingToPay','ConfirmLoan',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T20:03:37.746769Z",
     "start_time": "2018-06-26T20:03:37.736166Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate(comment, from_lang, to_lang):\n",
    "        if hasattr(comment, \"decode\"):\n",
    "            comment = comment.decode(\"utf-8\")\n",
    "\n",
    "        text = TextBlob(comment)\n",
    "        try:\n",
    "            text = text.translate(to=to_lang)\n",
    "            text = text.translate(to=from_lang)\n",
    "        except NotTranslated:\n",
    "            pass\n",
    "\n",
    "        return str(text)\n",
    "    \n",
    "\n",
    "\n",
    "def translate_csv(df,col,from_lang,to_lang,num_pol=10):\n",
    "    \"\"\"\n",
    "        https://developers.google.com/translate/v2/using_rest#language-params\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    if num_pol >= 1000:\n",
    "        num_pol=1000\n",
    "    comment_pool = df[col].values\n",
    "    p = Pool(num_pol)\n",
    "    new_col_name = col + '_' + to_lang\n",
    "    df[new_col_name] = p.starmap(translate, zip(comment_pool, repeat(from_lang),repeat(to_lang)))\n",
    "    df = df.drop([col], axis = 1)\n",
    "    df = df.rename(index=str, columns={new_col_name:col})\n",
    "    return df\n",
    "\n",
    "def cut_words(text):\n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    return \" \".join(seg_list)\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(f'([{string.punctuation}“”¨«»®´·º ½¾¿¡§£₤‘’，])',' ', text)\n",
    "    text = text.split(' ')\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def clean_label(label):\n",
    "    return int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#########################################\n",
    "#Goold Official Translation API\n",
    "#Need to Pay\n",
    "#############################################\n",
    "\n",
    "#################\n",
    "\n",
    "# # service = build('translate', 'v2', developerKey='AIzaSyD2ZNI7MBVVFdMnHse7w0asv2R3LyEFMf4')    # Jiangning\n",
    "# import html\n",
    "\n",
    "# service = build('translate', 'v2', developerKey='AIzaSyD2ZNI7MBVVFdMnHse7w0asv2R3LyEFMf4')\n",
    "\n",
    "# def translate2(values, from_lang, to_lang):\n",
    "#         if not isinstance(values, list):\n",
    "#             values = list(values)\n",
    "#         response = service.translations().list(source=from_lang,\n",
    "#                           target=to_lang, q=values).execute()\n",
    "#         processed = []\n",
    "#         for each in response['translations']:\n",
    "#             processed.append(each['translatedText'])\n",
    "#         processed = list(map(lambda x: html.unescape(x), processed))\n",
    "        \n",
    "#         # translate back\n",
    "#         response = service.translations().list(source=to_lang,\n",
    "#                           target=from_lang, q=processed).execute()\n",
    "#         processed = []\n",
    "#         for each in response['translations']:\n",
    "#             processed.append(each['translatedText'])\n",
    "#         return processed\n",
    "    \n",
    "    \n",
    "# def translate_csv(df,col,from_lang,to_lang,num_pol=None):\n",
    "#     \"\"\"\n",
    "#         https://developers.google.com/translate/v2/using_rest#language-params\n",
    "#     \"\"\"\n",
    "\n",
    "#     df = df.copy()\n",
    "#     new_col_name = col + '_' + to_lang\n",
    "#     df[new_col_name] = translate2(df[col].values, from_lang, to_lang)\n",
    "# #     df = df.drop([col], axis = 1)\n",
    "# #     df = df.rename(index=str, columns={new_col_name:col})\n",
    "#     return df\n",
    "\n",
    "# # c= translate_csv(data,col='split_text' ,from_lang='zh', to_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T20:05:28.293409Z",
     "start_time": "2018-06-26T20:05:27.427937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CutDebt\n",
      "finish cutting words\n",
      "finish shuffling\n",
      "IDClassifier\n",
      "finish cutting words\n",
      "finish shuffling\n",
      "IfKnowDebtor\n",
      "finish cutting words\n",
      "finish shuffling\n",
      "Installment\n",
      "finish cutting words\n",
      "finish shuffling\n",
      "WillingToPay\n",
      "finish cutting words\n",
      "finish shuffling\n",
      "ConfirmLoan\n",
      "finish cutting words\n",
      "finish shuffling\n"
     ]
    }
   ],
   "source": [
    "# # combine without translate\n",
    "# for each_model in model_list:\n",
    "#     print(each_model)\n",
    "#     path = '../../data/{}/'\n",
    "#     data = pd.read_csv(path.format(each_model) + 'combined_mock_up_data.csv', encoding = 'utf8')\n",
    "#     data = data.dropna()\n",
    "#     col = 'split_text'\n",
    "    \n",
    "#     # cut words\n",
    "#     data['split_text']=data['split_text'].apply(cut_words)\n",
    "#     print('finish cutting words')\n",
    "    \n",
    "#     # cleaning and save\n",
    "#     data['split_text'] = data['split_text'].apply(clean)\n",
    "#     data['label'] = data['label'].apply(clean_label)\n",
    "\n",
    "#     # shuffle data\n",
    "#     data = data.sample(frac=1).reset_index(drop=True)\n",
    "#     print('finish shuffling')\n",
    "#     data.to_csv(path.format(each_model) + 'cleaned_mock_up_data.csv', index = False, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T20:04:07.703588Z",
     "start_time": "2018-06-26T20:04:05.002062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CutDebt\n",
      "finish loading\n"
     ]
    },
    {
     "ename": "MaybeEncodingError",
     "evalue": "Error sending result: '<multiprocessing.pool.ExceptionWithTraceback object at 0x7fdb4c58dd68>'. Reason: 'TypeError(\"cannot serialize '_io.BufferedReader' object\",)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMaybeEncodingError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d96cd0a6b326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# translate and get more data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdata_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrom_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_pol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finish 1st trans'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdata_fr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrom_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_pol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-cfea02db7b1d>\u001b[0m in \u001b[0;36mtranslate_csv\u001b[0;34m(df, col, from_lang, to_lang, num_pol)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_pol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mnew_col_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mto_lang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_col_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mnew_col_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         '''\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaybeEncodingError\u001b[0m: Error sending result: '<multiprocessing.pool.ExceptionWithTraceback object at 0x7fdb4c58dd68>'. Reason: 'TypeError(\"cannot serialize '_io.BufferedReader' object\",)'"
     ]
    }
   ],
   "source": [
    "for each_model in model_list:\n",
    "    print(each_model)\n",
    "    path = '../../data/{}/'\n",
    "    data = pd.read_csv(path.format(each_model) + 'combined_mock_up_data.csv', encoding='utf8')\n",
    "    data = data.dropna()\n",
    "    col = 'split_text'\n",
    "    print('finish loading')\n",
    "    \n",
    "    # translate and get more data\n",
    "    data_en = translate_csv(data,col,from_lang='zh',to_lang='en',num_pol=50)\n",
    "    print('finish 1st trans')\n",
    "    data_fr = translate_csv(data,col,from_lang='zh',to_lang='fr',num_pol=50)\n",
    "    print('finish 2nd trans')\n",
    "    data_th = translate_csv(data,col,from_lang='zh',to_lang='th',num_pol=50)\n",
    "    print('finish 3rd trans')\n",
    "    data_lo = translate_csv(data,col,from_lang='zh',to_lang='lo',num_pol=50)\n",
    "    print('finish 4th trans')\n",
    "    data = pd.concat([data,data_en,data_fr,data_th,data_lo], ignore_index=True)\n",
    "#     data = pd.concat([data,data_en,data_fr], ignore_index=True)\n",
    "    \n",
    "    # cut words\n",
    "    data['split_text']=data['split_text'].apply(cut_words)\n",
    "    print('finish cutting words')\n",
    "    \n",
    "    # cleaning and save\n",
    "    data['split_text'] = data['split_text'].apply(clean)\n",
    "    data['label'] = data['label'].apply(clean_label)\n",
    "\n",
    "    # shuffle data\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    print('finish shuffling')\n",
    "    data.to_csv(path.format(each_model) + 'cleaned_mock_up_data.csv', index = False, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T20:06:10.696991Z",
     "start_time": "2018-06-26T20:06:10.575884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this part combine the data which can be used together.\n",
    "for each_model in model_list:\n",
    "    path = '../../data/{}/'\n",
    "    data = pd.read_csv(path.format(each_model) + 'mock_up_data1.csv', encoding='utf8')\n",
    "    data.to_csv(path.format(each_model) + 'combined_mock_up_data.csv', index = False, encoding = 'utf8')\n",
    "\n",
    "# label 0 part for CutDebt and Installment\n",
    "data_cut = pd.read_csv(path.format('CutDebt') + 'combined_mock_up_data.csv', encoding = 'utf8')\n",
    "data_ins = pd.read_csv(path.format('Installment') + 'combined_mock_up_data.csv', encoding = 'utf8')\n",
    "temp_cut = data_cut[data_cut.label == 0]\n",
    "temp_ins = data_ins[data_ins.label == 0]\n",
    "data_cut = pd.concat([data_cut,temp_ins], ignore_index=True)\n",
    "data_ins = pd.concat([data_ins,temp_cut], ignore_index=True)\n",
    "data_cut.to_csv(path.format('CutDebt') + 'combined_mock_up_data.csv', index = False, encoding = 'utf8')\n",
    "data_cut.to_csv(path.format('Installment') + 'combined_mock_up_data.csv', index = False, encoding = 'utf8')\n",
    "\n",
    "del data_cut\n",
    "del data_ins\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T14:56:10.877407Z",
     "start_time": "2018-06-26T14:56:10.815645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2751, 2)\n",
      "(2830, 2)\n",
      "(3763, 2)\n",
      "(2832, 2)\n",
      "(3278, 3)\n",
      "(3285, 2)\n"
     ]
    }
   ],
   "source": [
    "# # this script combine data in data1, exclude and data0621 together (jun 26 version)\n",
    "# with_exclude = ['CutDebt','Installment','WillingToPay','SetDueDay']\n",
    "# without_exclude = ['IDClassifier','IfKnowDebtor']\n",
    "\n",
    "# for each_model in with_exclude:\n",
    "#     path = '../../data/{}/'\n",
    "#     data1 = pd.read_csv(path.format(each_model) + 'mock_up_data1.csv', encoding = 'utf8')\n",
    "#     data0621 = pd.read_csv(path.format(each_model) + 'mock_up_data0621.csv', encoding = 'utf8')\n",
    "#     data_exclude = pd.read_csv(path.format(each_model) + 'mock_up_data_exludes.csv', encoding = 'utf8')\n",
    "#     data = pd.concat([data1,data0621,data_exclude], ignore_index=True)\n",
    "#     print(data.shape)\n",
    "#     data.to_csv(path.format(each_model) + 'mock_up_data.csv', index = False, encoding = 'utf8')\n",
    "    \n",
    "# for each_model in without_exclude:\n",
    "#     path = '../../data/{}/'\n",
    "#     data1 = pd.read_csv(path.format(each_model) + 'mock_up_data1.csv', encoding = 'utf8')\n",
    "#     data0621 = pd.read_csv(path.format(each_model) + 'mock_up_data0621.csv', encoding = 'utf8')\n",
    "#     data = pd.concat([data1,data0621,data_exclude], ignore_index=True)\n",
    "#     print(data.shape)\n",
    "#     data.to_csv(path.format(each_model) + 'mock_up_data.csv', index = False, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T16:02:13.387945Z",
     "start_time": "2018-06-26T16:02:13.358378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3278, 3)\n",
      "(3285, 2)\n",
      "(15, 2)\n"
     ]
    }
   ],
   "source": [
    "# # this script combine data in combine_list and 'setdueday' together to get new dataset\n",
    "\n",
    "# combine_list =  ['CutDebt','Installment','WillingToPay']\n",
    "\n",
    "# for each_model in combine_list:\n",
    "#     path = '../../data/{}/'\n",
    "#     SDD_data = pd.read_csv(path.format('SetDueDay') + 'mock_up_data.csv', encoding='utf8')\n",
    "#     data = pd.read_csv(path.format(each_model) + 'mock_up_data.csv', encoding='utf8')\n",
    "#     data_comb = pd.concat([data, SDD_data], ignore_index=True)\n",
    "#     print(data_comb.shape)\n",
    "#     data_comb.to_csv(path.format(each_model) + 'mock_up_data1.csv', index = False, encoding = 'utf8')\n",
    "\n",
    "\n",
    "# combine_list =  ['IDClassifier', 'IfKnowDebtor', 'ConfirmLoan']\n",
    "\n",
    "# for each_model in combine_list:\n",
    "#     path = '../../data/{}/'\n",
    "# #     SDD_data = pd.read_csv(path.format('SetDueDay') + 'mock_up_data.csv', encoding='utf8')\n",
    "#     data = pd.read_csv(path.format(each_model) + 'mock_up_data.csv', encoding='utf8')\n",
    "# #     data_comb = pd.concat([data, SDD_data], ignore_index=True)\n",
    "#     print(data.shape)\n",
    "#     data.to_csv(path.format(each_model) + 'mock_up_data1.csv', index = False, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
