{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T14:53:02.541339Z",
     "start_time": "2018-07-19T14:53:01.660342Z"
    }
   },
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "# import lightgbm as lgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import sys,os\n",
    "sys.path.append('../../MLModel/code/OneClickTraining/')\n",
    "from all_model_py import CutDebt, IDClassifier, IfKnowDebtor, Installment, WillingToPay, ConfirmLoan\n",
    "\n",
    "sys.path.append('../../Lib/')\n",
    "from model_matrix import eval_mat\n",
    "from SUPPORT import balance_category\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T14:53:02.919109Z",
     "start_time": "2018-07-19T14:53:02.914448Z"
    }
   },
   "outputs": [],
   "source": [
    "def sub_df(df,sets,target='label'):\n",
    "    result = pd.DataFrame()\n",
    "    for each in sets:\n",
    "        result = pd.concat([result,df[df[target]==each]])\n",
    "#     print(result[target].value_counts())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T14:53:03.427545Z",
     "start_time": "2018-07-19T14:53:03.386395Z"
    }
   },
   "outputs": [],
   "source": [
    "others = pd.read_csv('../../MLModel/data/others/cleaned_mock_up_data.csv')\n",
    "other_matrix = pd.read_csv('../../MLModel/data/others/strategy_mat.csv')\n",
    "target = 'label'\n",
    "save_path = '../../MLModel/savedModel/{}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CutDebt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T14:53:04.630319Z",
     "start_time": "2018-07-19T14:53:04.625310Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_list = {'CutDebt':CutDebt,\n",
    "              'IDClassifier':IDClassifier,\n",
    "              'IfKnowDebtor':IfKnowDebtor,\n",
    "              'Installment':Installment,\n",
    "              'ConfirmLoan':ConfirmLoan,\n",
    "              'WillingToPay':WillingToPay}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T14:58:31.303799Z",
     "start_time": "2018-07-19T14:58:12.480557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "model = 'CutDebt'\n",
    "df = pd.read_csv('../../MLModel/data/{}/cleaned_mock_up_data.csv'.format(model))\n",
    "other_label = int(max(set(df.label)) + 1)\n",
    "# filter out other label\n",
    "\n",
    "# get availabel other labels\n",
    "other_set = set(other_matrix[other_matrix[model]==0].label.values)\n",
    "ava_others = sub_df(others,other_set)\n",
    "ava_others[target] = other_label\n",
    "ava_others = ava_others.rename({'text':'split_text'},axis=1)\n",
    "df = pd.concat([df,ava_others],sort=True)\n",
    "# df = balance_category(df,target='label')\n",
    "df = df.sample(frac=1,random_state=21).reset_index(drop=True)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=False, random_state=None)\n",
    "ss = kf.split(df)\n",
    "result = pd.DataFrame()\n",
    "mat_lvc = []\n",
    "mat_lgr = []\n",
    "count = 0\n",
    "\n",
    "for train_index,val_index in ss:\n",
    "    count+=1\n",
    "    print(count)\n",
    "    train_df = df.iloc[train_index].copy()\n",
    "    train_df = balance_category(train_df,target='label')\n",
    "    val_df = df.iloc[val_index].copy()\n",
    "    train_data = train_df.split_text.values\n",
    "    val_data = val_df.split_text.values\n",
    "    phrase_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                strip_accents='unicode', \n",
    "                                max_features=100000, \n",
    "                                analyzer='word',\n",
    "                                sublinear_tf=True,\n",
    "                                token_pattern=r'\\w{1,}')\n",
    "\n",
    "\n",
    "    phrase_vectorizer.fit(train_data)\n",
    "    train_tfidf = phrase_vectorizer.transform(train_data)\n",
    "    val_tfidf = phrase_vectorizer.transform(val_data)\n",
    "\n",
    "    # linear svc\n",
    "    l_svc = LinearSVC()\n",
    "    lsvc = CalibratedClassifierCV(l_svc) \n",
    "    lsvc.fit(train_tfidf, train_df.label)\n",
    "    preds = lsvc.predict(val_tfidf)\n",
    "    mat = eval_mat(val_df.label.values, preds)\n",
    "    mat_lvc.append(mat.values)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T14:57:59.300465Z",
     "start_time": "2018-07-19T14:57:59.280312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[332.        ,   2.8       ,  27.5       ,   0.9165647 ],\n",
       "       [  8.        , 171.3       ,  53.4       ,   0.73398122],\n",
       "       [ 28.2       ,  47.2       , 504.9       ,   0.87031717],\n",
       "       [  0.90136335,   0.77380851,   0.86221053,   0.85782342]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mat_lvc,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T14:58:32.005843Z",
     "start_time": "2018-07-19T14:58:31.929751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[331.6       ,   3.1       ,  27.6       ,   0.91545661],\n",
       "       [  6.9       , 180.        ,  45.8       ,   0.77146112],\n",
       "       [ 27.8       ,  55.2       , 497.3       ,   0.85719352],\n",
       "       [  0.90491515,   0.75487173,   0.87166406,   0.85841866]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mat_lvc,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "======== Linear SVC =======\n",
    "               pred_0      pred_1      pred_2    recall\n",
    "actual_0   192.000000   10.000000    11.00000  0.901408\n",
    "actual_1     6.000000  116.000000    26.00000  0.783784\n",
    "actual_2    49.000000   22.000000  1231.00000  0.945469\n",
    "precision    0.777328    0.783784     0.97082  0.925436\n",
    "======== logistic =======\n",
    "               pred_0     pred_1      pred_2    recall\n",
    "actual_0   141.000000   8.000000    64.00000  0.661972\n",
    "actual_1     6.000000  69.000000    73.00000  0.466216\n",
    "actual_2    16.000000   8.000000  1278.00000  0.981567\n",
    "precision    0.865031   0.811765     0.90318  0.894768\n",
    "======== Naive Bayes =======\n",
    "           pred_0  pred_1       pred_2    recall\n",
    "actual_0     78.0    2.00   133.000000  0.366197\n",
    "actual_1      0.0   47.00   101.000000  0.317568\n",
    "actual_2      0.0    1.00  1301.000000  0.999232\n",
    "precision     1.0    0.94     0.847557  0.857486\n",
    "======== SVM =======\n",
    "               pred_0      pred_1       pred_2    recall\n",
    "actual_0   185.000000    9.000000    19.000000  0.868545\n",
    "actual_1     5.000000  107.000000    36.000000  0.722973\n",
    "actual_2    41.000000   15.000000  1246.000000  0.956989\n",
    "precision    0.800866    0.816794     0.957725  0.924835\n",
    "======== Random Forest =======\n",
    "           pred_0     pred_1       pred_2    recall\n",
    "actual_0   162.00   6.000000    45.000000  0.760563\n",
    "actual_1     9.00  93.000000    46.000000  0.628378\n",
    "actual_2    29.00  12.000000  1261.000000  0.968510\n",
    "precision    0.81   0.837838     0.932692  0.911606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# params={'task':'train','objective':'multiclass','num_class':3,}\n",
    "\n",
    "# train_set = lgb.Dataset(phrase_train,train.label.values)\n",
    "# model = lgb.train(params=params,train_set=train_set)\n",
    "# val_pred = model.predict(phrase_val)\n",
    "# val_pred = np.argmax(val_pred,axis=1)\n",
    "# evl = eval_mat(val.label.values, val_pred)\n",
    "# print('======== lightgbm =======')\n",
    "# print(evl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T04:01:52.059708Z",
     "start_time": "2018-07-19T04:01:52.051937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6006"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('6006')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'ConfirmLoan'\n",
    "df = pd.read_csv('../../MLModel/data/{}/cleaned_mock_up_data.csv'.format(model))\n",
    "other_label = int(max(set(df.label)) + 1)\n",
    "# filter out other label\n",
    "\n",
    "# get availabel other labels\n",
    "other_set = set(other_matrix[other_matrix[model]==0].label.values)\n",
    "ava_others = sub_df(others,other_set)\n",
    "ava_others[target] = other_label\n",
    "ava_others = ava_others.rename({'text':'split_text'},axis=1)\n",
    "df = pd.concat([df,ava_others],sort=True)\n",
    "df = balance_category(df,target='label')\n",
    "df = df.sample(frac=1,random_state=21).reset_index(drop=True)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=False, random_state=None)\n",
    "ss = kf.split(data)\n",
    "result = pd.DataFrame()\n",
    "mat_lvc = []\n",
    "\n",
    "for train_index,val_index in ss:\n",
    "    train_df = data.iloc[train_index].copy()\n",
    "#         train_df = balance_category(train_df,target='label')\n",
    "    val_df = data.iloc[val_index].copy()\n",
    "    train_data = train_df.split_text.values\n",
    "    val_data = val_df.split_text.values\n",
    "    phrase_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                strip_accents='unicode', \n",
    "                                max_features=100000, \n",
    "                                analyzer='word',\n",
    "                                sublinear_tf=True,\n",
    "                                token_pattern=r'\\w{1,}')\n",
    "\n",
    "\n",
    "    phrase_vectorizer.fit(train_data)\n",
    "    train_tfidf = phrase_vectorizer.transform(train_data)\n",
    "    val_tfidf = phrase_vectorizer.transform(val_data)\n",
    "\n",
    "    # linear svc\n",
    "    l_svc = LinearSVC()\n",
    "    lsvc = CalibratedClassifierCV(l_svc) \n",
    "    lsvc.fit(train_tfidf, train_df.label)\n",
    "    preds = lsvc.predict(val_tfidf)\n",
    "    mat = eval_mat(val_df.label.values, preds)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "print('=====  {} ======='.format(model))\n",
    "print(df.label.value_counts())\n",
    "print('begin training!')\n",
    "train,val = train_test_split(df,test_size=0.3,train_size=0.7,random_state=19)\n",
    "\n",
    "\n",
    "\n",
    "# get tfidf\n",
    "phrase_vectorizer = TfidfVectorizer(\n",
    "                                ngram_range=(1,3),\n",
    "                                strip_accents='unicode', \n",
    "                                max_features=100000, \n",
    "                                analyzer='word',\n",
    "                                sublinear_tf=True,\n",
    "                                use_idf=True,\n",
    "                                norm='l2',\n",
    "                                token_pattern=r'\\w{1,}')\n",
    "\n",
    "print('fitting phrase')\n",
    "phrase_vectorizer.fit(train.split_text)\n",
    "\n",
    "print('transform phrase')\n",
    "phrase_train = phrase_vectorizer.transform(train.split_text)\n",
    "phrase_val = phrase_vectorizer.transform(val.split_text)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "l_svc = LinearSVC(C=1)\n",
    "lsvc = CalibratedClassifierCV(l_svc) \n",
    "lsvc.fit(phrase_train, train.label)\n",
    "val_pred = lsvc.predict(phrase_val)\n",
    "evl = eval_mat(val.label.values, val_pred)\n",
    "print('======== Linear SVC =======')\n",
    "print(evl)\n",
    "\n",
    "\n",
    "# logistic\n",
    "log_r = LogisticRegression()\n",
    "log_r.fit(phrase_train, train.label)\n",
    "val_pred = log_r.predict(phrase_val)\n",
    "evl = eval_mat(val.label.values, val_pred)\n",
    "print('======== logistic =======')\n",
    "print(evl)\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "naive_b = MultinomialNB()\n",
    "naive_b.fit(phrase_train, train.label)\n",
    "val_pred = naive_b.predict(phrase_val)\n",
    "evl = eval_mat(val.label.values, val_pred)\n",
    "print('======== Naive Bayes =======')\n",
    "print(evl)\n",
    "\n",
    "# SVM\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(phrase_train, train.label)\n",
    "val_pred = svm.predict(phrase_val)\n",
    "evl = eval_mat(val.label.values, val_pred)\n",
    "print('======== SVM =======')\n",
    "print(evl)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(phrase_train, train.label)\n",
    "val_pred = rf.predict(phrase_val)\n",
    "evl = eval_mat(val.label.values, val_pred)\n",
    "print('======== Random Forest =======')\n",
    "print(evl)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
